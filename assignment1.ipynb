{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE4WC2_4wygJ"
   },
   "source": [
    "# Assignment 1\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Sexism Detection, Multi-class Classification, RNNs, Transformers, Huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL69zGpmx01k"
   },
   "source": [
    "# Contact\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "- Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "- Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "- Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55jnW-xKxi-2"
   },
   "source": [
    "# Introduction\n",
    "You are asked to address the [EXIST 2023 Task 2](https://clef2023.clef-initiative.eu/index.php?page=Pages/labs.html#EXIST) on sexism detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HWp5bGwySsb"
   },
   "source": [
    "## Problem Definition\n",
    "\n",
    "This task aims to categorize the sexist messages according to the intention of the author in one of the following categories: (i) direct sexist message, (ii) reported sexist message and (iii) judgemental message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8XKLLN26ofz"
   },
   "source": [
    "### Examples:\n",
    "\n",
    "#### DIRECT\n",
    "The intention was to write a message that is sexist by itself or incites to be sexist, as in:\n",
    "\n",
    "''*A woman needs love, to fill the fridge, if a man can give this to her in return for her services (housework, cooking, etc), I don’t see what else she needs.*''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGByBmBH6of0"
   },
   "source": [
    "#### REPORTED\n",
    "The intention is to report and share a sexist situation suffered by a woman or women in first or third person, as in:\n",
    "\n",
    "''*Today, one of my year 1 class pupils could not believe he’d lost a race against a girl.*''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e97QJ4Cn6of0"
   },
   "source": [
    "#### JUDGEMENTAL\n",
    "The intention was to judge, since the tweet describes sexist situations or behaviours with the aim of condemning them.\n",
    "\n",
    "''*As usual, the woman was the one quitting her job for the family’s welfare…*''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dependencies installation\n",
    "\n",
    "In the following, we will assume that you have \n",
    "- created a local python virtual environment - either with python [venv](https://docs.python.org/3/library/venv.html) module or via [uv](https://github.com/astral-sh/uv) (preferred) - with the `ipykernel` or `jupyter` packages pre-installed to start the jupyter kernel;\n",
    "- a stable internet connection\n",
    "Nothing else is needed.\n",
    "\n",
    "We will now download the `pyproject.toml` file specifying the project dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_project_root() -> Path:\n",
    "    \"\"\"Return the root directory of the project.\"\"\"\n",
    "    start_dir = Path.cwd()\n",
    "\n",
    "    markers = [\"assignment2.ipynb\"]\n",
    "\n",
    "    for path in [start_dir, *list(start_dir.parents)]:\n",
    "        for marker in markers:\n",
    "            if (path / marker).exists():\n",
    "                return path\n",
    "\n",
    "    return start_dir\n",
    "\n",
    "\n",
    "project_root: Path = get_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_repo: str = \"mpreda01/nlp-assignments\"\n",
    "project_branch: str = \"main\"\n",
    "pyproject_url = (\n",
    "    f\"https://raw.githubusercontent.com/{project_repo}/{project_branch}/pyproject.toml\"\n",
    ")\n",
    "lockfile_url = (\n",
    "    f\"https://raw.githubusercontent.com/{project_repo}/{project_branch}/uv.lock\"\n",
    ")\n",
    "urllib.request.urlretrieve(pyproject_url, project_root / \"pyproject.toml\")  # noqa: S310\n",
    "urllib.request.urlretrieve(lockfile_url, project_root / \"uv.lock\");  # noqa: S310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using [uv](https://github.com/astral-sh/uv) (recommended) you can now install the dependencies to a local virtual environment at `.venv` simply via\n",
    "```sh\n",
    "uv sync --extra assignment1\n",
    "```\n",
    "\n",
    "If not, the same can be achieved with the usual python [venv](https://docs.python.org/3/library/venv.html):\n",
    "```sh\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "(.venv) pip install \".[assignment1]\" \n",
    "```\n",
    "\n",
    "Make sure to do the above and *restart the kernel* if necessary before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iu1X4I98M8B"
   },
   "source": [
    "# [Task 1 - 1.0 points] Corpus\n",
    "\n",
    "We have preparared a small version of EXIST dataset in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material/tree/main/2025-2026/Assignment%201/data).\n",
    "\n",
    "Check the `A1/data` folder. It contains 3 `.json` files representing `training`, `validation` and `test` sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AASoMV9XN5l6"
   },
   "source": [
    "### Dataset Description\n",
    "- The dataset contains tweets in both English and Spanish.\n",
    "- There are labels for multiple tasks, but we are focusing on **Task 2**.\n",
    "- For Task 2, labels are assigned by six annotators.\n",
    "- The labels for Task 2 represent whether the tweet is non-sexist ('-') or its sexist intention ('DIRECT', 'REPORTED', 'JUDGEMENTAL').\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFjwB_lCOQKj"
   },
   "source": [
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "    \"203260\": {\n",
    "        \"id_EXIST\": \"203260\",\n",
    "        \"lang\": \"en\",\n",
    "        \"tweet\": \"ik when mandy says “you look like a whore” i look cute as FUCK\",\n",
    "        \"number_annotators\": 6,\n",
    "        \"annotators\": [\"Annotator_473\", \"Annotator_474\", \"Annotator_475\", \"Annotator_476\", \"Annotator_477\", \"Annotator_27\"],\n",
    "        \"gender_annotators\": [\"F\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n",
    "        \"age_annotators\": [\"18-22\", \"23-45\", \"18-22\", \"23-45\", \"46+\", \"46+\"],\n",
    "        \"labels_task1\": [\"YES\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\"],\n",
    "        \"labels_task2\": [\"DIRECT\", \"DIRECT\", \"REPORTED\", \"-\", \"JUDGEMENTAL\", \"REPORTED\"],\n",
    "        \"labels_task3\": [\n",
    "          [\"STEREOTYPING-DOMINANCE\"],\n",
    "          [\"OBJECTIFICATION\"],\n",
    "          [\"SEXUAL-VIOLENCE\"],\n",
    "          [\"-\"],\n",
    "          [\"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\"],\n",
    "          [\"OBJECTIFICATION\"]\n",
    "        ],\n",
    "        \"split\": \"TRAIN_EN\"\n",
    "      }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJ45bvuOOJ7I"
   },
   "source": [
    "### Instructions\n",
    "1. **Download** the `A1/data` folder.\n",
    "2. **Load** the three JSON files and encode them as ``pandas.DataFrame``.\n",
    "3. **Aggregate labels** for Task 2 using majority voting and store them in a new dataframe column called `label`. Items without a clear majority will be removed from the dataset.\n",
    "4. **Filter the DataFrame** to keep only rows where the `lang` column is `'en'`.\n",
    "5. **Remove unwanted columns**: Keep only `id_EXIST`, `lang`, `tweet`, and `label`.\n",
    "6. **Encode the `label` column**: Use the following mapping\n",
    "\n",
    "```\n",
    "{\n",
    "    '-': 0,\n",
    "    'DIRECT': 1,\n",
    "    'JUDGEMENTAL': 2,\n",
    "    'REPORTED': 3\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbCC9BsT6of2"
   },
   "outputs": [],
   "source": [
    "# File management\n",
    "import os\n",
    "\n",
    "# Utilities\n",
    "import random\n",
    "\n",
    "# NLP and Text Processing\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib\n",
    "import urllib.request\n",
    "import warnings\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import HfApi, login, repo_exists, whoami\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Machine Learning - Sklearn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Transformers and HuggingFace\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhTUbQJAMcy-"
   },
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # 0=all, 1=info, 2=warning, 3=error\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo: str = \"nlp-unibo/nlp-course-material\"\n",
    "branch: str = \"main\"\n",
    "folder_path: str = \"2025-2026/Assignment%201/data\"\n",
    "data_dir: Path = project_root / \"data\"\n",
    "files = [\"training.json\", \"test.json\", \"validation.json\"]\n",
    "\n",
    "base_url = f\"https://raw.githubusercontent.com/{repo}/{branch}/{folder_path}\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for filename in files:\n",
    "    url = f\"{base_url}/{filename}\"\n",
    "    destination = data_dir / f\"{filename}\"\n",
    "    urllib.request.urlretrieve(url, destination)  # noqa: S310\n",
    "    print(f\"Saved to: {destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lpi1S5kGFM5C"
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(data_dir / \"training.json\", orient=\"index\")\n",
    "val = pd.read_json(data_dir / \"validation.json\", orient=\"index\")\n",
    "test = pd.read_json(data_dir / \"test.json\", orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drgYeBXJ6of3"
   },
   "outputs": [],
   "source": [
    "def majority_vote(labels):\n",
    "    \"\"\"Apply majority voting to get the label with strict majority (>50%)\"\"\"\n",
    "    if not isinstance(labels, list) or len(labels) == 0:\n",
    "        return None\n",
    "    top_label, freq = Counter(labels).most_common(1)[0]\n",
    "    return top_label if freq > len(labels) / 2 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "Qghr8SLe6of3",
    "outputId": "fa55a512-e233-4165-f03f-a7c3f4acee2c"
   },
   "outputs": [],
   "source": [
    "# Label mapping for Task 2\n",
    "label_map = {\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3}\n",
    "\n",
    "# Process train, validation, and test sets\n",
    "for name in (\"train\", \"val\", \"test\"):\n",
    "    df = globals()[name].copy()\n",
    "\n",
    "    # Step 1: Aggregate labels using majority voting\n",
    "    df[\"label\"] = df[\"labels_task2\"].apply(majority_vote)\n",
    "\n",
    "    # Remove items without a clear majority\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "    # Step 2: Filter to keep only English rows\n",
    "    df = df[df[\"lang\"] == \"en\"]\n",
    "\n",
    "    # Step 3: Keep only required columns\n",
    "    df = df[[\"id_EXIST\", \"lang\", \"tweet\", \"label\"]]\n",
    "\n",
    "    # Step 4: Encode the label column\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "    # Update the global variable\n",
    "    globals()[name] = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train set shape: {train.shape}\")\n",
    "print(f\"Validation set shape: {val.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")\n",
    "print(\n",
    "    f\"\\nLabel distribution in train set:\\n{train['label'].value_counts().sort_index()}\"\n",
    ")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8T8xngn6of4"
   },
   "source": [
    "# [Task2 - 0.5 points] Data Cleaning\n",
    "In the context of tweets, we have noisy and informal data that often includes unnecessary elements like emojis, hashtags, mentions, and URLs. These elements may interfere with the text analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn3eBmPZ6of4"
   },
   "source": [
    "\n",
    "### Instructions\n",
    "- **Remove emojis** from the tweets.\n",
    "- **Remove hashtags** (e.g., `#example`).\n",
    "- **Remove mentions** such as `@user`.\n",
    "- **Remove URLs** from the tweets.\n",
    "- **Remove special characters and symbols**.\n",
    "- **Remove specific quote characters** (e.g., curly quotes).\n",
    "- **Perform lemmatization** to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bupcwlnN6of4",
    "outputId": "8847f68f-3010-47df-af48-0423d4434328"
   },
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank POS tags to WordNet POS tags for better lemmatization\"\"\"\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN  # Default to noun\n",
    "\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \"\"\"Clean tweet text by removing noise and performing lemmatization\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove mentions (@user)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # Remove hashtags (#example)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s\\-\\.\\,\\!\\?\\']\", \"\", text)\n",
    "\n",
    "    # Remove specific quote characters (curly quotes, etc.)\n",
    "    text = re.sub(r'[\"\"' \"`´]\", '\"', text)  # noqa: RUF001\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove special characters and short tokens, then lemmatize\n",
    "    cleaned_tokens = []\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    for token, pos in pos_tags:\n",
    "        # Skip if token is too short or only special characters\n",
    "        if len(token) < 2:  # noqa: PLR2004\n",
    "            continue\n",
    "        # Lemmatize using POS tag\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        lemmatized = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
    "        cleaned_tokens.append(lemmatized)\n",
    "\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "\n",
    "# Apply cleaning to all datasets\n",
    "for name in (\"train\", \"val\", \"test\"):\n",
    "    print(f\"Cleaning {name} set...\")\n",
    "    globals()[name][\"tweet\"] = globals()[name][\"tweet\"].apply(clean_tweet)\n",
    "\n",
    "print(\"\\nData cleaning completed!\")\n",
    "print(f\"\\nSample cleaned tweets from train set:\")\n",
    "for i in range(min(3, len(train))):\n",
    "    print(f\"{i + 1}. {train['tweet'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3KylLHNl0bE"
   },
   "source": [
    "# [Task 3 - 0.5 points] Text Encoding\n",
    "To train a neural sexism classifier, you first need to encode text into numerical format.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr1lTHUVOXff"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6NNMEjWOZQr"
   },
   "source": [
    "### What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe **must** be added to the vocabulary.\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **special token** (e.g., ``<UNK>``) and a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90UztlGUObXk"
   },
   "source": [
    "### More about OOV\n",
    "\n",
    "For a given token:\n",
    "\n",
    "* **If in train set**: add to vocabulary and assign an embedding (use GloVe if token in GloVe, custom embedding otherwise).\n",
    "* **If in val/test set**: assign special token if not in vocabulary and assign custom embedding.\n",
    "\n",
    "Your vocabulary **should**:\n",
    "\n",
    "* Contain all tokens in train set; or\n",
    "* Union of tokens in train set and in GloVe $\\rightarrow$ we make use of existing knowledge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFK0AXE_6of5",
    "outputId": "c4565f11-4621-4921-bc08-d7b885925bca"
   },
   "outputs": [],
   "source": [
    "# GloVe embedding dimension\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Create embeddings directory\n",
    "embeddings_dir: Path = project_root / \"embeddings\"\n",
    "if not os.path.exists(embeddings_dir):\n",
    "    embeddings_dir.mkdir()\n",
    "\n",
    "# Download GloVe embeddings (6B tokens, 100d)\n",
    "glove_file = embeddings_dir / f\"glove.6B.{EMBEDDING_DIM}d.txt\"\n",
    "\n",
    "if not os.path.exists(glove_file):\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    zip_path = embeddings_dir / \"glove.6B.zip\"\n",
    "    urllib.request.urlretrieve(url, zip_path)  # noqa: S310\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(embeddings_dir)  # noqa: S202\n",
    "    os.remove(zip_path)\n",
    "    print(\"Done!\")\n",
    "\n",
    "# Load GloVe embeddings into a dictionary\n",
    "print(f\"Loading GloVe embeddings ({EMBEDDING_DIM}d)...\")\n",
    "glove_embeddings = {}\n",
    "with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype=\"float32\")\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "print(f\"Loaded {len(glove_embeddings)} word embeddings from GloVe\")\n",
    "\n",
    "# Build vocabulary from training set\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "vocab = {}\n",
    "vocab_idx = 0\n",
    "\n",
    "# Special tokens\n",
    "vocab[\"<PAD>\"] = vocab_idx\n",
    "vocab_idx += 1\n",
    "vocab[\"<UNK>\"] = vocab_idx\n",
    "vocab_idx += 1\n",
    "\n",
    "# Collect all unique tokens from training data\n",
    "train_tokens = set()\n",
    "for tweet in train[\"tweet\"]:\n",
    "    tokens = tweet.split()\n",
    "    train_tokens.update(tokens)\n",
    "\n",
    "print(f\"Tokens in training set: {len(train_tokens)}\")\n",
    "\n",
    "# First, add all training tokens to vocabulary\n",
    "for token in sorted(train_tokens):\n",
    "    if token not in vocab:\n",
    "        vocab[token] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "# Optionally add GloVe tokens not in training (union approach)\n",
    "# This enriches our vocabulary with words that might appear in val/test\n",
    "glove_tokens_added = 0\n",
    "# Sample 10% of GloVe tokens not in training to keep vocabulary manageable\n",
    "glove_sample = np.random.choice(  # noqa: NPY002\n",
    "    list(set(glove_embeddings.keys()) - train_tokens),\n",
    "    size=min(50000, len(glove_embeddings) // 10),\n",
    "    replace=False,\n",
    ")\n",
    "for token in sorted(glove_sample):\n",
    "    if token not in vocab:\n",
    "        vocab[token] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "        glove_tokens_added += 1\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"  - Training tokens: {len(train_tokens)}\")\n",
    "print(f\"  - GloVe tokens added: {glove_tokens_added}\")\n",
    "\n",
    "# Create embedding matrix\n",
    "print(\"\\nCreating embedding matrix...\")\n",
    "embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIM))\n",
    "\n",
    "# Initialize embeddings\n",
    "oov_count = 0\n",
    "glove_count = 0\n",
    "\n",
    "for token, idx in vocab.items():\n",
    "    if token in glove_embeddings:\n",
    "        # Token found in GloVe - use pre-trained embedding\n",
    "        embedding_matrix[idx] = glove_embeddings[token]\n",
    "        glove_count += 1\n",
    "    elif token == \"<PAD>\":  # noqa: S105\n",
    "        # PAD token gets zero vector (will be masked)\n",
    "        embedding_matrix[idx] = np.zeros(EMBEDDING_DIM)\n",
    "    elif token == \"<UNK>\":  # noqa: S105\n",
    "        # UNK token gets random initialization\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=EMBEDDING_DIM)  # noqa: NPY002\n",
    "    else:\n",
    "        # OOV tokens in training set get random initialization\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=EMBEDDING_DIM)  # noqa: NPY002\n",
    "        oov_count += 1\n",
    "\n",
    "print(f\"Embeddings from GloVe: {glove_count}\")\n",
    "print(f\"Custom embeddings (OOV training tokens): {oov_count}\")\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# Create reverse vocabulary (index to token)\n",
    "idx_to_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "# Function to encode tweets for validation and test sets\n",
    "def encode_tweet(tweet, vocab):\n",
    "    \"\"\"\n",
    "    Encode a tweet to token indices.\n",
    "\n",
    "    Strategy for tokens:\n",
    "    - If token in vocabulary: use its index\n",
    "    - If token NOT in vocabulary (OOV in val/test): map to <UNK> token\n",
    "\n",
    "    This ensures all val/test tokens get an embedding (either their own or <UNK>)\n",
    "    \"\"\"\n",
    "    tokens = tweet.split()\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            indices.append(vocab[token])\n",
    "        else:\n",
    "            # Token not in vocabulary -> map to <UNK>\n",
    "            indices.append(vocab[\"<UNK>\"])\n",
    "    return indices\n",
    "\n",
    "\n",
    "# Encode validation and test sets using the token-to-index mapping\n",
    "print(\"\\nEncoding datasets...\")\n",
    "train[\"encoded_tweet\"] = train[\"tweet\"].apply(lambda x: encode_tweet(x, vocab))\n",
    "val[\"encoded_tweet\"] = val[\"tweet\"].apply(lambda x: encode_tweet(x, vocab))\n",
    "test[\"encoded_tweet\"] = test[\"tweet\"].apply(lambda x: encode_tweet(x, vocab))\n",
    "\n",
    "print(\"Datasets encoded successfully!\")\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary Coverage:\")\n",
    "print(f\"  - Vocabulary size: {len(vocab)}\")\n",
    "print(f\"  - GloVe coverage: {glove_count / len(vocab) * 100:.1f}%\")\n",
    "print(f\"  - Custom coverage: {oov_count / len(vocab) * 100:.1f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nExample vocab and embeddings:\")\n",
    "for token in [\"the\", \"woman\", \"sexist\", \"<UNK>\", \"<PAD>\"]:\n",
    "    if token in vocab:\n",
    "        idx = vocab[token]\n",
    "        print(f\"  {token}: index={idx}, embedding_dim={embedding_matrix[idx].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JLnuLGHGAUT"
   },
   "source": [
    "# [Task 4 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your sexism classifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQFI9J-JOfXD"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "\n",
    "* **Stacked**: add an additional Bidirectional LSTM layer to the Baseline model.\n",
    "\n",
    "**Note**: You are **free** to experiment with hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jALc_qYGS2E"
   },
   "source": [
    "### Token to embedding mapping\n",
    "\n",
    "You can follow two approaches for encoding tokens in your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zlz1II66of6"
   },
   "source": [
    "### Work directly with embeddings\n",
    "\n",
    "- Compute the embedding of each input token\n",
    "- Feed the mini-batches of shape ``(batch_size, # tokens, embedding_dim)`` to your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPOxLHo16of6"
   },
   "source": [
    "### Work with Embedding layer\n",
    "\n",
    "- Encode input tokens to token ids\n",
    "- Define a Embedding layer as the first layer of your model\n",
    "- Compute the embedding matrix of all known tokens (i.e., tokens in your vocabulary)\n",
    "- Initialize the Embedding layer with the computed embedding matrix\n",
    "- You are **free** to set the Embedding layer trainable or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJxGD9pA4dGf"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 100\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 4  # 0: non-sexist, 1: direct, 2: judgemental, 3: reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XkK7vHNw6of6",
    "outputId": "41cb5d3c-63d5-4b4a-d05d-be07c642ab58"
   },
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "print(\"Padding sequences...\")\n",
    "train_padded = pad_sequences(\n",
    "    train[\"encoded_tweet\"], maxlen=MAX_SEQ_LENGTH, padding=\"post\"\n",
    ")\n",
    "val_padded = pad_sequences(val[\"encoded_tweet\"], maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
    "test_padded = pad_sequences(\n",
    "    test[\"encoded_tweet\"], maxlen=MAX_SEQ_LENGTH, padding=\"post\"\n",
    ")\n",
    "\n",
    "print(f\"Padded sequences shape: {train_padded.shape}\")\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train[\"label\"].values\n",
    "y_val = val[\"label\"].values\n",
    "y_test = test[\"label\"].values\n",
    "\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Val labels shape: {y_val.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "# Function to create Baseline model\n",
    "def create_baseline_model(  # noqa: PLR0913\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    max_seq_length,  # noqa: ARG001\n",
    "    embedding_matrix,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create Baseline model: Embedding -> Bidirectional LSTM -> Dense\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        embedding_dim: Embedding dimension\n",
    "        max_seq_length: Maximum sequence length\n",
    "        embedding_matrix: Pre-trained embedding matrix\n",
    "        lstm_units: Number of LSTM units\n",
    "        dropout_rate: Dropout rate\n",
    "        num_classes: Number of output classes\n",
    "        learning_rate: Learning rate for optimizer\n",
    "\n",
    "    Returns:\n",
    "        Compiled model\n",
    "    \"\"\"\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False,  # Keep GloVe embeddings frozen\n",
    "                name=\"embedding\",\n",
    "            ),\n",
    "            Bidirectional(\n",
    "                LSTM(units=lstm_units, return_sequences=False), name=\"bilstm_1\"\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(units=num_classes, activation=\"softmax\", name=\"output\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to create Stacked model\n",
    "def create_stacked_model(  # noqa: PLR0913\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    max_seq_length,  # noqa: ARG001\n",
    "    embedding_matrix,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create Stacked model: Embedding -> BiLSTM -> BiLSTM -> Dense\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        embedding_dim: Embedding dimension\n",
    "        max_seq_length: Maximum sequence length\n",
    "        embedding_matrix: Pre-trained embedding matrix\n",
    "        lstm_units: Number of LSTM units per layer\n",
    "        dropout_rate: Dropout rate\n",
    "        num_classes: Number of output classes\n",
    "        learning_rate: Learning rate for optimizer\n",
    "\n",
    "    Returns:\n",
    "        Compiled model\n",
    "    \"\"\"\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False,  # Keep GloVe embeddings frozen\n",
    "                name=\"embedding\",\n",
    "            ),\n",
    "            Bidirectional(\n",
    "                LSTM(units=lstm_units, return_sequences=True), name=\"bilstm_1\"\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            Bidirectional(\n",
    "                LSTM(units=lstm_units, return_sequences=False), name=\"bilstm_2\"\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(units=num_classes, activation=\"softmax\", name=\"output\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "stacked_model = create_stacked_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "print(\"\\nBASELINE MODEL ARCHITECTURE:\")\n",
    "baseline_model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nSTACKED MODEL ARCHITECTURE:\")\n",
    "stacked_model.summary()\n",
    "\n",
    "# Configuration summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"LSTM Units: {LSTM_UNITS}\")\n",
    "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFjBgdiRG3wD"
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline and Stacked models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWPK4umGOjtT"
   },
   "source": [
    "\n",
    "### Instructions\n",
    "\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation and test sets.\n",
    "* Compute macro F1-score, precision, and recall metrics on the validation set.\n",
    "* Report average and standard deviation measures over seeds for each metric.\n",
    "* Pick the **best** performing model according to the observed validation set performance (use macro F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-m2Zz3j6of7"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)  # noqa: NPY002\n",
    "\n",
    "    # TensorFlow - both global and operation level\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)  # Additional TF seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "020ce852ce614fcc9cd7316388a6a3f6",
      "86bf5bc5d8c54c8ca6c2feb43f025039",
      "e6fd43f6babc48329181a17586c47d14",
      "6006a836147e4f2892321311d3b92369",
      "b50b9c45dee2412396091af8f6506011",
      "7bc5bcd73e3243bdb416acf0d8c9df42",
      "2ef0544df27d4608bd1c663610f7bda6",
      "cef9da2ebc5b4ab2999adcbb3f810226",
      "be6c43f56d5f4ceb992604c64b47d182",
      "a57777f14d794bf7860743f993d28722",
      "22885fbd6d7743d5abc20dc825d22349",
      "8d0c4bf3bac248b1b933f9a4bf9e0792",
      "b89b41718cf34b49a2df11d53471b60e",
      "bc94b64f86234abf807ec66bb186da11",
      "45360f149b5c444da7bb40dadbfd95bf",
      "8de2d7355a8248baa8bbaf1f79819117",
      "f8d9f47dda8841d0a95ba357bf391b90",
      "46c0dc0213934848b1c1b774a047f5f8",
      "5e1e4de8f0ca44c0a04c15450505be2e",
      "92b4edd669694bddad8839462195a20f",
      "9550927b950d4d6dbd47d8182d361f6a",
      "6509aa29c4b94761ab56a761d8edbbbc",
      "482c960434dd418e963edbbdc0b295fc",
      "b0cb25691be24b1f9caa9b256b87c353",
      "65aede485ea8426fa6ebef549c0cd5f4",
      "e4ed90664eb14fe3955472e4a631a327",
      "fca8e2792b11497aa200fa883d3ddf0b",
      "786632c9b0d74fd781d18c8e33d8ee8e",
      "9cf427517eb7405cb7a08d0e5c4f6fca",
      "6fe858782ad1453189a2113e34bcc0fb",
      "75dafb66b23e449eb1af755c8c0d3ec0",
      "5468b10a62814174a610fce611ce1a85",
      "adf9b353100d45eead0e8bae732efda6",
      "23fbde51085940d993caba7fbd67a8ef",
      "cad56dfb57aa42e8acbaa202f98590e2",
      "b4cfa65b95bf4543a6c6bfaa4095557d",
      "91025d25ed364dfc8c71f7fcfbc9b8cb",
      "48511f620740441e8fc0804796bee5a0",
      "93e1e3787f834b87bd6483b6c94ca52f",
      "f0372b9867cb4f799cd9c13b60dd30b9",
      "5a0c14f4e85149c7b5d6b8ac17e3ade6",
      "763a4dda69eb40229c8ffc58efce4fa8",
      "1cc56b860c554d6781a298f80158723a",
      "341c1525e6834f0f9ce9d9ed7871dab2"
     ]
    },
    "id": "VRlAQ_YqMvib",
    "outputId": "d488d1a6-8481-43d4-94f7-6082354234fc"
   },
   "outputs": [],
   "source": [
    "f1_metric = evaluate.load(\"f1\")\n",
    "pre_metric = evaluate.load(\"precision\")\n",
    "rec_metric = evaluate.load(\"recall\")\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(output_info):\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    pre = pre_metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"macro\"\n",
    "    )\n",
    "    rec = rec_metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"macro\"\n",
    "    )\n",
    "\n",
    "    return {**acc, **f1, **pre, **rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6yal8HfHNwV"
   },
   "outputs": [],
   "source": [
    "SEEDS = [42, 53, 82]\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RN06-ay06of8",
    "outputId": "c626cb1e-ca3f-48a0-fbef-7d9d3e7af73d"
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "training_histories = {}  # Store training histories for plotting\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING WITH SEED: {seed}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    set_seeds(seed)\n",
    "\n",
    "    models[seed] = {}\n",
    "    training_histories[seed] = {}\n",
    "\n",
    "    # Re-create models for each seed\n",
    "    models[seed][\"baseline_model\"] = create_baseline_model(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "    models[seed][\"stacked_model\"] = create_stacked_model(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "    # Train Baseline model\n",
    "    print(\"\\nTraining Baseline Model...\")\n",
    "    training_histories[seed][\"baseline\"] = models[seed][\"baseline_model\"].fit(\n",
    "        train_padded,\n",
    "        y_train,\n",
    "        validation_data=(val_padded, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Train Stacked model\n",
    "    print(\"\\nTraining Stacked Model...\")\n",
    "    training_histories[seed][\"stacked\"] = models[seed][\"stacked_model\"].fit(\n",
    "        train_padded,\n",
    "        y_train,\n",
    "        validation_data=(val_padded, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cis1TYjn6of8",
    "outputId": "b83ff326-1028-48f6-f2b8-0a312da8fcdc"
   },
   "outputs": [],
   "source": [
    "# Compute metrics on validation set for each model and seed\n",
    "val_metrics = {}\n",
    "val_metrics = {\"baseline_model\": {}, \"stacked_model\": {}}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    # Baseline Model Validation Metrics\n",
    "    val_metrics[\"baseline_model\"][seed] = {}\n",
    "\n",
    "    baseline_pred = models[seed][\"baseline_model\"].predict(val_padded, verbose=0)\n",
    "    val_metrics[\"baseline_model\"][seed] = compute_metrics((baseline_pred, y_val))\n",
    "\n",
    "    # Stacked Model Validation Metrics\n",
    "    val_metrics[\"stacked_model\"][seed] = {}\n",
    "\n",
    "    stacked_pred = models[seed][\"stacked_model\"].predict(val_padded, verbose=0)\n",
    "    val_metrics[\"stacked_model\"][seed] = compute_metrics((stacked_pred, y_val))\n",
    "\n",
    "# Compute average and std dev across seeds\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AVERAGE METRICS ACROSS SEEDS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_f1_scores = [val_metrics[\"baseline_model\"][seed][\"f1\"] for seed in SEEDS]\n",
    "baseline_precision_scores = [\n",
    "    val_metrics[\"baseline_model\"][seed][\"precision\"] for seed in SEEDS\n",
    "]\n",
    "baseline_recall_scores = [\n",
    "    val_metrics[\"baseline_model\"][seed][\"recall\"] for seed in SEEDS\n",
    "]\n",
    "\n",
    "stacked_f1_scores = [val_metrics[\"stacked_model\"][seed][\"f1\"] for seed in SEEDS]\n",
    "stacked_precision_scores = [\n",
    "    val_metrics[\"stacked_model\"][seed][\"precision\"] for seed in SEEDS\n",
    "]\n",
    "stacked_recall_scores = [val_metrics[\"stacked_model\"][seed][\"recall\"] for seed in SEEDS]\n",
    "\n",
    "print(\"\\nBaseline Model:\")\n",
    "print(\n",
    "    f\"  F1-Score (macro):     {np.mean(baseline_f1_scores):.4f} ± {np.std(baseline_f1_scores):.4f}\"  # noqa: E501\n",
    ")\n",
    "print(\n",
    "    f\"  Precision (macro):    {np.mean(baseline_precision_scores):.4f} ± {np.std(baseline_precision_scores):.4f}\"  # noqa: E501\n",
    ")\n",
    "print(\n",
    "    f\"  Recall (macro):       {np.mean(baseline_recall_scores):.4f} ± {np.std(baseline_recall_scores):.4f}\"  # noqa: E501\n",
    ")\n",
    "\n",
    "print(\"\\nStacked Model:\")\n",
    "print(\n",
    "    f\"  F1-Score (macro):     {np.mean(stacked_f1_scores):.4f} ± {np.std(stacked_f1_scores):.4f}\"  # noqa: E501\n",
    ")\n",
    "print(\n",
    "    f\"  Precision (macro):    {np.mean(stacked_precision_scores):.4f} ± {np.std(stacked_precision_scores):.4f}\"  # noqa: E501\n",
    ")\n",
    "print(\n",
    "    f\"  Recall (macro):       {np.mean(stacked_recall_scores):.4f} ± {np.std(stacked_recall_scores):.4f}\"  # noqa: E501\n",
    ")\n",
    "\n",
    "# Determine best model\n",
    "best_baseline_f1 = np.max(baseline_f1_scores)\n",
    "best_stacked_f1 = np.max(stacked_f1_scores)\n",
    "best_model_type = \"Baseline\" if best_baseline_f1 >= best_stacked_f1 else \"Stacked\"\n",
    "best_f1_score = max(best_baseline_f1, best_stacked_f1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"BEST MODEL: {best_model_type} (Macro F1-Score: {best_f1_score:.4f})\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jTRfff82scSa",
    "outputId": "8bc0d62d-f92c-4df4-eb84-0031a213546c"
   },
   "outputs": [],
   "source": [
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "\n",
    "# Create figure with 4 subplots: Baseline Loss, Baseline Acc, Stacked Loss, Stacked Acc\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"LSTM Models - Learning Curves\", fontsize=16, y=0.995)\n",
    "\n",
    "# BASELINE LOSS - All seeds overlaid\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    history = training_histories[seed][\"baseline\"]\n",
    "    axes[0, 0].plot(\n",
    "        history.history[\"loss\"],\n",
    "        label=f\"Seed {seed} - Train\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "    )\n",
    "    axes[0, 0].plot(\n",
    "        history.history[\"val_loss\"],\n",
    "        label=f\"Seed {seed} - Val\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "axes[0, 0].set_xlabel(\"Epoch\", fontsize=11)\n",
    "axes[0, 0].set_ylabel(\"Loss\", fontsize=11)\n",
    "axes[0, 0].set_title(\"Baseline LSTM - Loss\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 0].legend(fontsize=9, loc=\"upper right\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# BASELINE ACCURACY - All seeds overlaid\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    history = training_histories[seed][\"baseline\"]\n",
    "    axes[0, 1].plot(\n",
    "        history.history[\"accuracy\"],\n",
    "        label=f\"Seed {seed} - Train\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "    )\n",
    "    axes[0, 1].plot(\n",
    "        history.history[\"val_accuracy\"],\n",
    "        label=f\"Seed {seed} - Val\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "axes[0, 1].set_xlabel(\"Epoch\", fontsize=11)\n",
    "axes[0, 1].set_ylabel(\"Accuracy\", fontsize=11)\n",
    "axes[0, 1].set_title(\"Baseline LSTM - Accuracy\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0, 1].legend(fontsize=9, loc=\"lower right\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# STACKED LOSS - All seeds overlaid\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    history = training_histories[seed][\"stacked\"]\n",
    "    axes[1, 0].plot(\n",
    "        history.history[\"loss\"],\n",
    "        label=f\"Seed {seed} - Train\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "    )\n",
    "    axes[1, 0].plot(\n",
    "        history.history[\"val_loss\"],\n",
    "        label=f\"Seed {seed} - Val\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "axes[1, 0].set_xlabel(\"Epoch\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Loss\", fontsize=11)\n",
    "axes[1, 0].set_title(\"Stacked LSTM - Loss\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 0].legend(fontsize=9, loc=\"upper right\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# STACKED ACCURACY - All seeds overlaid\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    history = training_histories[seed][\"stacked\"]\n",
    "    axes[1, 1].plot(\n",
    "        history.history[\"accuracy\"],\n",
    "        label=f\"Seed {seed} - Train\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "    )\n",
    "    axes[1, 1].plot(\n",
    "        history.history[\"val_accuracy\"],\n",
    "        label=f\"Seed {seed} - Val\",\n",
    "        linewidth=2.5,\n",
    "        color=colors[seed_idx],\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "axes[1, 1].set_xlabel(\"Epoch\", fontsize=11)\n",
    "axes[1, 1].set_ylabel(\"Accuracy\", fontsize=11)\n",
    "axes[1, 1].set_title(\"Stacked LSTM - Accuracy\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 1].legend(fontsize=9, loc=\"lower right\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSy9sPwYHUoD"
   },
   "source": [
    "# [Task 6 - 1.0 points] Transformers\n",
    "\n",
    "In this section, you will use a transformer model specifically trained for hate speech detection, namely [Twitter-roBERTa-base for Hate Speech Detection](https://huggingface.co/cardiffnlp/twitter-roberta-base-hate).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS_XsNwX6of8"
   },
   "source": [
    "### Relevant Material\n",
    "- Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y29Ag-3p6of9"
   },
   "source": [
    "### Instructions\n",
    "- **Load the Tokenizer and Model**\n",
    "\n",
    "- **Preprocess the Dataset**:\n",
    "   You will need to preprocess your dataset to prepare it for input into the model. Tokenize your text data using the appropriate tokenizer and ensure it is formatted correctly.\n",
    "\n",
    "- **Train the Model**:\n",
    "   Use the `Trainer` to train the model on your training data.\n",
    "\n",
    "- **Evaluate the Model on the Test Set** using the same metrics used for LSTM-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created an account and an access token, we need to login to Huggingface via code. Add an environment variable `HF_TOKEN` with your token\n",
    "```sh\n",
    "export HF_TOKEN=\"my-huggingface-token-with-the-correct-access-rights\"\n",
    "```\n",
    "as [expected by huggingface](https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables#hftoken).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDxWMlss0--F",
    "outputId": "93914b07-c44d-4a90-f45a-092c10c0fb11"
   },
   "outputs": [],
   "source": [
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    try:\n",
    "        login(token=hf_token, add_to_git_credential=True)\n",
    "        user = whoami(token=hf_token)\n",
    "        username = user[\"name\"]\n",
    "        print(f\"Logged in as @{username}\")\n",
    "        print(f\"Profile page: https://huggingface.co/{username}\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid token.\")\n",
    "    except ImportError:\n",
    "        print(\"ipywidgets not installed.\")\n",
    "else:\n",
    "    print(\"Warning: 'HF_TOKEN' environment variable not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te5Z3taf0--G"
   },
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "repo_id = \"nlp-transformer-weights\"\n",
    "\n",
    "# Check if repo already exists\n",
    "if repo_exists(repo_id):\n",
    "    print(f\"Repository '{repo_id}' already exists\")\n",
    "else:\n",
    "    print(f\"Creating repository '{repo_id}'...\")\n",
    "    api.create_repo(repo_id=repo_id)\n",
    "    print(f\"Repository created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "a3e35d2b293546a89c65a27a18b2150d",
      "af0a521c1a254d02bc122e57a0f5d9a0",
      "3bfb1c5577284066a1e1897c4e1a0459",
      "7cccca7b33224a048279fecf1c32b98c",
      "d0fb8f77f97d469f8dfd5f1caedc7412",
      "29f6c0c48f064bf3aa19179ed914b64b",
      "49d1a353e47547a1b392a11e7ad31089",
      "3b58d81427e043f79839a3ef3950d585",
      "ebab7239e25f4767aaa127a44c3dc8ac",
      "3a3140831aa44914837955c3b42ba479",
      "cb249ef65474490fa75f3a6167938a4e",
      "bcfa2f22e21340d1acb3fec84305b0eb",
      "f3c20f0cecfe4f2c9a7ffa2489afc6c9",
      "737e3f2fecff47c0b67983c0598b6d38",
      "471ff3cf6b12436f8bdfac85a433520b",
      "86f163df32cc4a0cbad93e28443d315c",
      "00dff725334547efbbc275917cad3b16",
      "cd000d5cf59c4a7b8f8724ec76c117a9",
      "cc74e14bd634483bad91d17f9015c13b",
      "eaf91faacee345eca24705e7e41dc701",
      "1f47d5509446425e81baa3fffd273b7b",
      "8bf4bcd7908e47c9b3bb69221d93b91b",
      "5e8bab0768594ce5965d6dbb9c4133b1",
      "dfafdfee34ec4f709a0213feed114df8",
      "ba880525556649009aae74d58a5f2038",
      "99536f0ea54f4d5f8adc77f04b2d4573",
      "ac1167e6f7d94080a7d52138944e3dbc",
      "8495fd4def184338bd49de26ef8ccc29",
      "092a9e951cfa4a7b9a8744995184a898",
      "19f975e9f9f345d398a1dd35c28bdd45",
      "de204f9be7b5408b804c0d42e32d4338",
      "dfca7562f26d4b17b6b9cb36a9543e40",
      "2d75982a042d4b9cb3ad65d0c45d0c4d",
      "6124a52b15b14a3690eab78c7c0c85ee",
      "e110837924c145cbb36ca0b41f9dbf66",
      "8ee39d93c1e742edb84320bf678afaec",
      "2c6743e0d09e4844abd805385347876f",
      "f0bb63d6d8d94afaa230e5e2fced755b",
      "27182ea32c974b43abc1e54c86430de2",
      "8951c0081273449e98e47e939d4ee8a3",
      "1054e304765d4e0ebdc978efa38c5c01",
      "96be25cc2f93489d9206ad633cf10d6b",
      "557b65b6cf1746b28f85f6e510a4e69b",
      "9fdb28befad149d6b0765a3fc7165420"
     ]
    },
    "id": "mpc8AZa-6of9",
    "outputId": "21ddeb6a-3869-443f-96eb-8fe4d5e2c874"
   },
   "outputs": [],
   "source": [
    "task = \"hate\"\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Tokenize the datasets\n",
    "encoded_train_raw = tokenizer(train[\"tweet\"].tolist(), padding=True)\n",
    "encoded_val_raw = tokenizer(val[\"tweet\"].tolist(), padding=True)\n",
    "encoded_test_raw = tokenizer(test[\"tweet\"].tolist(), padding=True)\n",
    "\n",
    "# Convert to Hugging Face Dataset objects\n",
    "encoded_train = Dataset.from_dict(encoded_train_raw)\n",
    "encoded_val = Dataset.from_dict(encoded_val_raw)\n",
    "encoded_test = Dataset.from_dict(encoded_test_raw)\n",
    "\n",
    "# Add labels to the datasets\n",
    "encoded_train = encoded_train.add_column(\"labels\", train[\"label\"].tolist())\n",
    "encoded_val = encoded_val.add_column(\"labels\", val[\"label\"].tolist())\n",
    "encoded_test = encoded_test.add_column(\"labels\", test[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4m1JKDG86ogB",
    "outputId": "87fca428-1587-4ddb-f237-37b8cc31e8b4"
   },
   "outputs": [],
   "source": [
    "original_text = train[\"tweet\"][12]\n",
    "decoded_text = tokenizer.decode(encoded_train[\"input_ids\"][12])\n",
    "\n",
    "print(original_text[:200])\n",
    "print()\n",
    "print()\n",
    "print(decoded_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb50mHGGVr-t"
   },
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(train[\"label\"].tolist()), y=train[\"label\"].tolist()\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Custom Trainer\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs=False,\n",
    "        num_items_in_batch=None,  # noqa: ARG002\n",
    "    ):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Move class_weights to the same device as labels\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights.to(labels.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"nlp_assignment\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"  # dont log all model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation study for transformer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eUgWfS2-ggs"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING 4 TRANSFORMER MODEL VARIANTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dictionary to store all trained models\n",
    "transformer_models = {}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: STANDARD MODEL WITH DEFAULT SETTINGS (10 EPOCHS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 1: STANDARD TRANSFORMER (10 EPOCHS, DEFAULT)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create model\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    id2label={0: \"-\", 1: \"DIRECT\", 2: \"JUDGEMENTAL\", 3: \"REPORTED\"},\n",
    "    label2id={\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Training args for model 1 (default 10 epochs)\n",
    "training_args_1 = TrainingArguments(\n",
    "    output_dir=\"test_dir_model1\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    label_smoothing_factor=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ROBERTA-base\",\n",
    ")\n",
    "\n",
    "# Create trainer for model 1\n",
    "trainer_1 = Trainer(\n",
    "    model=model_1,\n",
    "    args=training_args_1,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training Model 1...\")\n",
    "trainer_1.train()\n",
    "transformer_models[\"model_1_standard_10epochs\"] = trainer_1\n",
    "wandb.finish()\n",
    "print(\"\\nModel 1 training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: STANDARD MODEL WITH MORE TRAINING (30 EPOCHS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 2: STANDARD TRANSFORMER (30 EPOCHS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model_2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    id2label={0: \"-\", 1: \"DIRECT\", 2: \"JUDGEMENTAL\", 3: \"REPORTED\"},\n",
    "    label2id={\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Training args for model 2 (30 epochs)\n",
    "training_args_2 = TrainingArguments(\n",
    "    output_dir=\"test_dir_model2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    label_smoothing_factor=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ROBERTA-30-epochs\",\n",
    ")\n",
    "\n",
    "# Create trainer for model 2\n",
    "trainer_2 = Trainer(\n",
    "    model=model_2,\n",
    "    args=training_args_2,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training Model 2...\")\n",
    "trainer_2.train()\n",
    "transformer_models[\"model_2_standard_30epochs\"] = trainer_2\n",
    "wandb.finish()\n",
    "print(\"\\nModel 2 training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: WEIGHTED LOSS TRAINER (WITH CLASS WEIGHTS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 3: WEIGHTED LOSS TRAINER (WITH CLASS WEIGHTS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model_3 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    id2label={0: \"-\", 1: \"DIRECT\", 2: \"JUDGEMENTAL\", 3: \"REPORTED\"},\n",
    "    label2id={\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Training args for model 3 (default 10 epochs)\n",
    "training_args_3 = TrainingArguments(\n",
    "    output_dir=\"test_dir_model3\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    label_smoothing_factor=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ROBERTA-class-weights\",\n",
    ")\n",
    "\n",
    "# Create weighted loss trainer for model 3\n",
    "trainer_3 = WeightedLossTrainer(\n",
    "    model=model_3,\n",
    "    args=training_args_3,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Training Model 3 with weighted loss...\")\n",
    "trainer_3.train()\n",
    "transformer_models[\"model_3_weighted_loss\"] = trainer_3\n",
    "wandb.finish()\n",
    "print(\"\\nModel 3 training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 4: FROZE LAYERS (FINE-TUNE JUST CLASSIFICATION HEAD)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 4: FROZE LAYERS (FINE-TUNE JUST CLASSIFICATION HEAD)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model_4 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    id2label={0: \"-\", 1: \"DIRECT\", 2: \"JUDGEMENTAL\", 3: \"REPORTED\"},\n",
    "    label2id={\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Unfreeze last 4 layers of the transformer + classification head\n",
    "# By default, we fine-tune all layers; here we explicitly ensure this\n",
    "for param in model_4.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Training args for model 4\n",
    "training_args_4 = TrainingArguments(\n",
    "    output_dir=\"test_dir_model4\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    label_smoothing_factor=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ROBERTA-just-head\",\n",
    ")\n",
    "\n",
    "# Create trainer for model 4\n",
    "trainer_4 = Trainer(\n",
    "    model=model_4,\n",
    "    args=training_args_4,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Model 4 with frozen layers...\")\n",
    "trainer_4.train()\n",
    "transformer_models[\"model_4_frozen_layers\"] = trainer_4\n",
    "wandb.finish()\n",
    "print(\"\\nModel 4 training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY AND MODEL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: MODEL COMPARISON ON VALIDATION SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "# Initialize a new W&B run for the evaluation phase\n",
    "wandb.init(project=\"nlp_assignment\", name=\"Transformer_Evaluation\")\n",
    "\n",
    "for model_name, trainer_obj in transformer_models.items():\n",
    "    # Get validation predictions\n",
    "    val_pred_info = trainer_obj.predict(encoded_val)\n",
    "    val_f1 = compute_metrics([val_pred_info.predictions, val_pred_info.label_ids])[\"f1\"]\n",
    "\n",
    "    model_results[model_name] = {\"f1_score\": val_f1, \"trainer\": trainer_obj}\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Validation F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "# Finish the W&B run after evaluation\n",
    "wandb.finish()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(model_results.items(), key=lambda x: x[1][\"f1_score\"])[0]\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {model_results[best_model_name]['f1_score']:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nSaving best model to Hugging Face Hub...\")\n",
    "transformer_models[best_model_name].push_to_hub(f\"best_model_{best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkcKwJiPGkOD"
   },
   "outputs": [],
   "source": [
    "eval_acc = pd.read_csv(\"eval_acc.csv\", sep=\";\")\n",
    "train_loss = pd.read_csv(\"train_loss.csv\", sep=\";\")\n",
    "eval_acc_30e = pd.read_csv(\"30e_eval_acc.csv\", sep=\";\")\n",
    "train_loss_30e = pd.read_csv(\"30e_train_loss.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54Y_WoUVGkOD",
    "outputId": "9d3ef51b-a6ed-463a-e7dc-da09f08fbbb4"
   },
   "outputs": [],
   "source": [
    "# Drop only NaN cells (individual values)\n",
    "eval_acc_clean = eval_acc.dropna(axis=0)\n",
    "train_loss_clean = train_loss.dropna(axis=0)\n",
    "eval_acc_30e_clean = eval_acc_30e.dropna(axis=0)\n",
    "train_loss_30e_clean = train_loss_30e.dropna(axis=0)\n",
    "\n",
    "# Create figure with 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot eval_acc\n",
    "eval_acc_clean.plot(ax=axes[0, 0], marker=\"o\")\n",
    "axes[0, 0].set_title(\"Evaluation Accuracy\")\n",
    "axes[0, 0].set_xlabel(\"Index\")\n",
    "axes[0, 0].set_ylabel(\"Accuracy\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot train_loss\n",
    "train_loss_clean.plot(ax=axes[0, 1], marker=\"o\")\n",
    "axes[0, 1].set_title(\"Training Loss\")\n",
    "axes[0, 1].set_xlabel(\"Index\")\n",
    "axes[0, 1].set_ylabel(\"Loss\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot eval_acc_30e\n",
    "eval_acc_30e_clean.plot(ax=axes[1, 0], marker=\"o\")\n",
    "axes[1, 0].set_title(\"Evaluation Accuracy (30 epochs)\")\n",
    "axes[1, 0].set_xlabel(\"Index\")\n",
    "axes[1, 0].set_ylabel(\"Accuracy\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot train_loss_30e\n",
    "train_loss_30e_clean.plot(ax=axes[1, 1], marker=\"o\")\n",
    "axes[1, 1].set_title(\"Training Loss (30 epochs)\")\n",
    "axes[1, 1].set_xlabel(\"Index\")\n",
    "axes[1, 1].set_ylabel(\"Loss\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqr41qC9GkOE"
   },
   "outputs": [],
   "source": [
    "eval_f1 = pd.read_csv(\"eval_f1.csv\", sep=\";\").dropna(axis=0)\n",
    "eval_f1_30e = pd.read_csv(\"30e_eval_f1.csv\", sep=\";\").dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dix6gfcHGkOE",
    "outputId": "59da1491-e01f-4997-c1d1-c113f61c1b56"
   },
   "outputs": [],
   "source": [
    "# Drop only NaN cells (individual values)\n",
    "eval_f1_clean = eval_f1.dropna(axis=0)\n",
    "eval_f1_30e_clean = eval_f1_30e.dropna(axis=0)\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot eval_f1\n",
    "eval_f1_clean.plot(ax=axes[0], marker=\"o\")\n",
    "axes[0].set_title(\"Evaluation F1-Score\")\n",
    "axes[0].set_xlabel(\"Index\")\n",
    "axes[0].set_ylabel(\"F1-Score\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot eval_f1_30e\n",
    "eval_f1_30e_clean.plot(ax=axes[1], marker=\"o\")\n",
    "axes[1].set_title(\"Evaluation F1-Score (30 epochs)\")\n",
    "axes[1].set_xlabel(\"Index\")\n",
    "axes[1].set_ylabel(\"F1-Score\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train of the best ablation model respect F1 score (macro) for different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-4gh1vMGkOF"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# Store models and trainers\n",
    "model_3_seeds = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\nTraining Model 3 with Seed {seed}...\")\n",
    "\n",
    "    # Create model\n",
    "    model_3 = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        id2label={0: \"-\", 1: \"DIRECT\", 2: \"JUDGEMENTAL\", 3: \"REPORTED\"},\n",
    "        label2id={\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3},\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    # Training args for model 3 (default 10 epochs)\n",
    "    training_args_3 = TrainingArguments(\n",
    "        output_dir=f\"test_dir_model3_{seed}\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        seed=seed,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=100,\n",
    "        label_smoothing_factor=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"ROBERTA-class-weights_seed{seed}\",\n",
    "    )\n",
    "\n",
    "    # Create weighted loss trainer for model 3\n",
    "    trainer_3 = WeightedLossTrainer(\n",
    "        model=model_3,\n",
    "        args=training_args_3,\n",
    "        train_dataset=encoded_train,\n",
    "        eval_dataset=encoded_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer_3.train()\n",
    "\n",
    "    api = HfApi()\n",
    "    repo_id = f\"nlp-transformer-weights_seed{seed}\"\n",
    "    api.create_repo(repo_id=repo_id)\n",
    "    print(f\"Repository created successfully\")\n",
    "\n",
    "    # Save to HuggingFace Hub\n",
    "    print(f\"Saving Model 3 (Seed {seed}) to HuggingFace Hub...\")\n",
    "    model_3.push_to_hub(f\"nlp-transformer-weights_seed{seed}\", private=False)\n",
    "\n",
    "    # Store the trainer\n",
    "    model_3_seeds[seed] = trainer_3\n",
    "\n",
    "    wandb.finish()\n",
    "    print(f\"Model 3 (Seed {seed}) training and saving completed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 3 TRAINING COMPLETE FOR ALL SEEDS\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMJB-BscFM5M"
   },
   "outputs": [],
   "source": [
    "seed_eval_acc = pd.read_csv(\"seed_eval_acc.csv\", sep=\";\")\n",
    "seed_train_loss = pd.read_csv(\"seed_train_loss.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6_9CBi6FM5M",
    "outputId": "b2426a31-ca93-4b40-ab67-08d68811e1e8"
   },
   "outputs": [],
   "source": [
    "# Drop NaN values for clean plotting\n",
    "seed_eval_acc_clean = seed_eval_acc.dropna(axis=0)\n",
    "seed_train_loss_clean = seed_train_loss.dropna(axis=0)\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot evaluation accuracy\n",
    "seed_eval_acc_clean.plot(ax=axes[0], marker=\"o\")\n",
    "axes[0].set_title(\"Evaluation Accuracy Across Seeds\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training loss\n",
    "seed_train_loss_clean.plot(ax=axes[1], marker=\"o\")\n",
    "axes[1].set_title(\"Training Loss Across Seeds\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305,
     "referenced_widgets": [
      "fec51768a75140b6b6711e9934fcfc6e",
      "711d0e8751bb4e21a19255fd08a64f31",
      "36bc99dd51b44b2f98ffe6895ad48c17",
      "b15c262a2400430b8e88c77685f68e54",
      "27a8d9a124db4713bc5b11806ceece32",
      "8203deef12f74254a2b325eed0e1b27d",
      "c99a038e41e34b0fa05ad491b57e8bbb",
      "fc15c936c8fe497d8427c951b015ecb6",
      "24d85ef3bf8f4f1ca08d4e042b0229e1",
      "8092cc97c4944f9582c3bf1436bec561",
      "662569926939444583d9dd4adf6c7259",
      "8c8dbf0f592d49409b810e23901745d9",
      "4246241e2e1343f5a60df1e0265e081c",
      "7f90153cdf3b48bd8880c163aefda33e",
      "fcd52e407e154c8f9b6edc95f421c3bd",
      "1701eeeaf87f488d9f689edf571c1759",
      "68cb65214e2d44c899e67009fef0a561",
      "7c94b6294a17401f9693cf07b8bd5f9b",
      "025e5b735ea64058afa49dda1f00c05b",
      "c8a2560bb55b40acbab71ab10ac62780",
      "0090fd04c0034a2d99ca5c09f5d92803",
      "e3d2360c50314f9881fd4630b2878aa3",
      "da57949dc821451ca9f987a202fa8956",
      "f26f1f289c34450d9e82e16e631f3a8c",
      "a70a53b42219450d9e880c9e1f153f17",
      "ac1c4074a6fb44c9b96f4e0fd156bed1",
      "a9b26bbb212b41cdb7a1c0249110b0d9",
      "4d3aaef1ae7e42f1baa0a59ea215e757",
      "e99f95e0f1264034b1521ecbd7d7b4a0",
      "ce8ea1d485c5425f8c39ef4539105d00",
      "cac89063b1764d799c9a43d854e99266",
      "854d20d0b74041f2a85fb1583222e029",
      "ab2c57197f5243fd99101228922b8303",
      "a0b03b7ee75041fdbfa79460fc679714",
      "bb4e7f2ed1b24fcf97bb82fae9299fd0",
      "59a6ad1c5e2f418e9958138a3832da3f",
      "be634f78d51746f4ad71832151302133",
      "359a326da9784464a390a9f0085fc0dc",
      "aae1b838e0674a9fb74c054ad41ccf91",
      "15e0809197d04799a0cf9989d48b3bb8",
      "74730d7c890340fd80ac64df7c09a985",
      "2703eb4b5d2640d09864acbb1d32a2d9",
      "757873241f6a4e4085fa574bb6913f07",
      "8346b06e3170467bbb742465625b5852",
      "e900410cbfaf4065a67fa381e0b0537f",
      "374f33f0c6954b698e5bf9453948e08d",
      "4a7b6c52c72a49748105e268311a3570",
      "67414e20f6dd4dc382700c09d2718870",
      "79f4ecc08eac45b89042e2d42e95a287",
      "9c49d7d88e12489aaf42c4204a0eec45",
      "7ab733f631bc42049091c925a145dae4",
      "087268ce9f1e40778534801f5b250614",
      "37e04799119f4d4284d5c7930dd44ab2",
      "35fd73e8ddb04305a91a8139a44703d0",
      "4f070942d1df4e1e87a6a11d22f7e5c3",
      "283081792026408f81a6318951343019",
      "261e82f78c5d4c9f864d3c46a1f42df9",
      "c96bb6143a1244b796523f0df469f232",
      "bfc3e791e41340ada6dfa9eb6e247856",
      "56e8916335294fd3b41d5629c2c164c9",
      "60ddae0947404996bfa23046864beed6",
      "d1a44a2bbabb45ac9b3e15ed57f723df",
      "88325431fbcc42179541189d9d204aea",
      "9544c877fda44c1aa068bd1891f510aa",
      "ada1a9c556984478845254b881139ce7",
      "55fc5dfa83e74635bcc70dcb673957cb",
      "ea6db987ee76406cbb134e59c1fc1bbb",
      "5016bca4f9934860aa72c31dfd3a4c10",
      "62c82a9b4b7b4b0eb3bc5ab1b8fd203a",
      "9fa2f462fcb94469a81f9456b27985a7",
      "b8d4f3b23991438dab87e8bc57949efd",
      "491ed0292fc245f5beb62046617e6059",
      "3b69430ac63947b7b98dfec3c866a196",
      "2cac37cc53d440e5b1cd0d7781218065",
      "95b68f5046984d689959ac0ca119ad0b",
      "fb6bdeba0c334afda3f4ba7768b78932",
      "dd103af3c6014efd808c5e867573fd13",
      "119d4e9bbf7d485382e97689505e641a",
      "67a38f1e3f844f768fc9050a1616b960",
      "88a10729fb4c4d6aa9ecc15fcb1b46ac",
      "08d3f60ce6d44597a02cb7b97b8eeb0c",
      "2648670ff6684236ab9928ed1531d397",
      "b6303a685e3e40329ab73104e2eb4e25",
      "40edfd8a91d84e82980bc6f37634eec5",
      "ae912d92a91341aea4b4a38a44d95f21",
      "4878196f1ec4431ba207be3ed744dd1a",
      "1f7a6ba793b542d5af23cf5d0c9f0196",
      "311d859fd052427f8d46705a205e41e3",
      "fd3733f075f84bcab461026677524bd3",
      "bb3540281e644192ac47d23b93bb626e",
      "f8f03822ded346ae9d0c40dd5f744105",
      "f9ed705e1071466cb70962ae443649ce",
      "d8175cb433094e0c85f3c88865505569",
      "a138395e98924a43949e37aa89fb6c0e",
      "72a9866fa6a244229320f99a10a7bb81",
      "e8c9694d073e4777be67937bee395f62",
      "6e1f7b91ee924a4ab9f8527baf7e2461",
      "c6118e959046423fae023c838366bb33",
      "6fdacd24087d4608997ab9c23287228d"
     ]
    },
    "id": "-CThUawmB30m",
    "outputId": "f4be5f3d-4f6c-4c4a-853d-44dbb6830084"
   },
   "outputs": [],
   "source": [
    "y_test = test[\"label\"].values\n",
    "seed_metrics = {}\n",
    "seed_predictions = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    repo_id_to_load = f\"mpreda/nlp-transformer-weights_seed{seed}\"\n",
    "\n",
    "    # Ensure distinct cache directory for each seed to prevent caching issues\n",
    "    unique_cache_dir = f\"./huggingface_cache_seed_{seed}\"\n",
    "    os.makedirs(unique_cache_dir, exist_ok=True)\n",
    "\n",
    "    # Load the model directly from the Hugging Face Hub\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        repo_id_to_load,\n",
    "        cache_dir=unique_cache_dir,  # Use a unique cache directory\n",
    "        force_download=True,  # Force download to bypass any local cache conflicts\n",
    "    )\n",
    "\n",
    "    # Ensure model is in evaluation mode and on the correct device\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Manually prepare inputs from encoded_test for direct model prediction\n",
    "    input_ids = torch.tensor(encoded_test[\"input_ids\"], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor(encoded_test[\"attention_mask\"], dtype=torch.long).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Make predictions (get logits) without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Move logits to CPU and convert to numpy for metric computation\n",
    "    test_predictions_seed = logits.cpu().numpy()\n",
    "    seed_predictions[seed] = test_predictions_seed\n",
    "\n",
    "    # Compute metrics using y_test\n",
    "    seed_metrics[seed] = compute_metrics([test_predictions_seed, y_test])\n",
    "\n",
    "    # Clean up model and GPU memory for the next iteration\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2co1KERMFM5N",
    "outputId": "a3e69aa3-83a5-45a1-9037-fa335dfb3f1b"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AVERAGE TRANSFORMER METRICS ACROSS SEEDS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "f1_mean = np.mean([seed_metrics[seed][\"f1\"] for seed in SEEDS])\n",
    "f1_std = np.std([seed_metrics[seed][\"f1\"] for seed in SEEDS])\n",
    "precision_mean = np.mean([seed_metrics[seed][\"precision\"] for seed in SEEDS])\n",
    "precision_std = np.std([seed_metrics[seed][\"precision\"] for seed in SEEDS])\n",
    "recall_mean = np.mean([seed_metrics[seed][\"recall\"] for seed in SEEDS])\n",
    "recall_std = np.std([seed_metrics[seed][\"recall\"] for seed in SEEDS])\n",
    "\n",
    "print(f\"  F1-Score (macro):     {f1_mean:.4f} \\u00b1 {f1_std:.4f}\")\n",
    "print(f\"  Precision (macro):    {precision_mean:.4f} \\u00b1 {precision_std:.4f}\")\n",
    "print(f\"  Recall (macro):       {recall_mean:.4f} \\u00b1 {recall_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLicUbw1Q5n7"
   },
   "source": [
    "## Multilingual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSTIf-mRFM5N"
   },
   "outputs": [],
   "source": [
    "train_multi = pd.read_json(\"data/training.json\", orient=\"index\")\n",
    "val_multi = pd.read_json(\"data/validation.json\", orient=\"index\")\n",
    "test_multi = pd.read_json(\"data/test.json\", orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVMxRIOdQ8hN",
    "outputId": "bec690ca-7591-4259-c643-ba929d862cd0"
   },
   "outputs": [],
   "source": [
    "# Label mapping for Task 2\n",
    "label_map = {\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3}\n",
    "\n",
    "# Process train, validation, and test sets - INCLUDING BOTH EN AND ES\n",
    "for name in (\"train_multi\", \"val_multi\", \"test_multi\"):\n",
    "    df = globals()[name].copy()\n",
    "\n",
    "    # Step 1: Aggregate labels using majority voting\n",
    "    df[\"label\"] = df[\"labels_task2\"].apply(majority_vote)\n",
    "\n",
    "    # Remove items without a clear majority\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "\n",
    "    # Step 2: Filter to keep BOTH English ('en') and Spanish ('es') rows\n",
    "    df = df[df[\"lang\"].isin([\"en\", \"es\"])]\n",
    "\n",
    "    # Step 3: Keep only required columns\n",
    "    df = df[[\"id_EXIST\", \"lang\", \"tweet\", \"label\"]]\n",
    "\n",
    "    # Step 4: Encode the label column\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "    # Update the global variable\n",
    "    globals()[name] = df.reset_index(drop=True)\n",
    "\n",
    "print(\"MULTILINGUAL DATA PREPROCESSING COMPLETED\")\n",
    "print(f\"Train set shape: {train_multi.shape}\")\n",
    "print(f\"Validation set shape: {val_multi.shape}\")\n",
    "print(f\"Test set shape: {test_multi.shape}\")\n",
    "\n",
    "print(f\"\\nLanguage distribution in train set:\")\n",
    "print(train_multi[\"lang\"].value_counts())\n",
    "\n",
    "print(f\"\\nLabel distribution in train set:\")\n",
    "print(train_multi[\"label\"].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nSample multilingual tweets:\")\n",
    "for i in range(min(3, len(train_multi))):\n",
    "    lang = train_multi[\"lang\"].iloc[i]\n",
    "    label = train_multi[\"label\"].iloc[i]\n",
    "    tweet = train_multi[\"tweet\"].iloc[i]\n",
    "    print(f\"{i + 1}. [{lang}] (label={label}): {tweet[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyqDdwQ4WmrF",
    "outputId": "f7904a6a-6259-4fb3-fc1a-b52a0c082f47"
   },
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank POS tags to WordNet POS tags for better lemmatization\"\"\"\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN  # Default to noun\n",
    "\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \"\"\"Clean tweet text by removing noise and performing lemmatization\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove mentions (@user)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # Remove hashtags (#example)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\w\\s\\-\\.\\,\\!\\?\\']\", \"\", text)\n",
    "\n",
    "    # Remove specific quote characters (curly quotes, etc.)\n",
    "    text = re.sub(r'[\"\"' \"`´]\", '\"', text)  # noqa: RUF001\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove special characters and short tokens, then lemmatize\n",
    "    cleaned_tokens = []\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    for token, pos in pos_tags:\n",
    "        # Skip if token is too short or only special characters\n",
    "        if len(token) < 2:  # noqa: PLR2004\n",
    "            continue\n",
    "        # Lemmatize using POS tag\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        lemmatized = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
    "        cleaned_tokens.append(lemmatized)\n",
    "\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "\n",
    "# Apply cleaning to all multilingual datasets\n",
    "print(\"Cleaning multilingual datasets...\")\n",
    "for name in (\"train_multi\", \"val_multi\", \"test_multi\"):\n",
    "    print(f\"Cleaning {name} set...\")\n",
    "    globals()[name][\"tweet\"] = globals()[name][\"tweet\"].apply(clean_tweet)\n",
    "\n",
    "print(\"\\nMultilingual data cleaning completed!\")\n",
    "print(f\"\\nSample cleaned multilingual tweets from train set:\")\n",
    "for i in range(min(3, len(train_multi))):\n",
    "    lang = train_multi[\"lang\"].iloc[i]\n",
    "    label = train_multi[\"label\"].iloc[i]\n",
    "    tweet = train_multi[\"tweet\"].iloc[i]\n",
    "    print(f\"{i + 1}. [{lang}] (label={label}): {tweet[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "6711dcdd983143569da57b1eee7c49bf",
      "cbaad75776694eae8fdcd6400a3edfe7",
      "10496e0542404619a9f9b0d0ecb2ee38",
      "76f3762aa89941d28a9ac22c0eea7471",
      "bb950818991343669cb8ac59ac4ddff8",
      "97bd828c16154c99b58d7f2dacbb654f",
      "d8f2c3b13df644ae809bffe4c9a35a23",
      "d47ede4f019344d9a98052e803f5c424",
      "fd5fa82ca9554abeb751f3d76607684a",
      "5056e3237d6543a28f5ae4de6cb9e58d",
      "898f41882faa4024a26e1d4b211a14fb",
      "1a23a7f295d84448b508a3690263d3c5",
      "56b194da1d994eccbcac84b46ff8fe57",
      "779a3b88317b462eae192b40d1998af2",
      "912f96ca5753434da3986546456ce708",
      "c0d6af4708f8422984dc2f6171b87c41",
      "1d44dd2586e942c29087a82741b410d1",
      "ac7d71858cf64859aa3f29ae5618d487",
      "1a0c4d3cd3ad47488fd8bdddb04e30cd",
      "54906e8fdf054e7eb036e4c9173e6365",
      "9defb034e3ee448abd75762466aab5fe",
      "e8169aa5abf6455eb4b2ec1482af4328",
      "da6da1be08d141fda77676581eb5a42b",
      "9b50559fc1e4404d8dcaccdca3178118",
      "9f2f1d6c6f3a4cfb9e11a7342d4643ed",
      "8e6d5ee3fd9a472eb469a24c4a007678",
      "d723840d8cc6443ab80f0dea34819bc1",
      "00250aa09bc94675ab9e1818f72d2998",
      "78ee0ea3d85444d18fe8f739ca67e451",
      "b9cf8ea5cadc4bd28de1e5dc0ca259ce",
      "ee04c85bedad49f199ca0e735c656986",
      "7db3ee317c56426793a242488a117e9c",
      "573de0a0a50e48baab405ad0036f34e5",
      "79812a305c114917b41491cc422b48b0",
      "2cc5797108d94f08960cdc736eb2464e",
      "b66caf30efcb4a4c9f3a1bf72bb2523b",
      "9a55d1dc015844aebbe447387e6bd3a1",
      "bdc0bfe8340f4f9ea801c6d50dffc34f",
      "ad9bab5f50c146dfb1a919fa68cdf09b",
      "02717630d4c843169fe0babc8cd49671",
      "7ca4611ff1ae40a5ab7377ad8434f7ca",
      "e62129828fbd400d997a957b7e9cb068",
      "94b2ecbe659a4d9bbdd2a60accb3cb5b",
      "3185d886830f43e8a5572226f66db6bf"
     ]
    },
    "id": "v2D5SaEWRDQS",
    "outputId": "b38f1b9c-a4fb-4356-908e-f35cfe8651c4"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "def encode_dataset(df, tokenizer, max_length=128):\n",
    "    \"\"\"Encode a dataframe using the tokenizer\"\"\"\n",
    "    encodings = tokenizer(\n",
    "        df[\"tweet\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Create a dataset with encodings and labels\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"input_ids\": encodings[\"input_ids\"].tolist(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].tolist(),\n",
    "            \"label\": df[\"label\"].tolist(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Encode all three datasets\n",
    "print(\"Encoding multilingual datasets...\")\n",
    "encoded_train_multi = encode_dataset(train_multi, tokenizer)\n",
    "encoded_val_multi = encode_dataset(val_multi, tokenizer)\n",
    "encoded_test_multi = encode_dataset(test_multi, tokenizer)\n",
    "\n",
    "print(f\"\\nEncoding completed!\")\n",
    "print(f\"Encoded train set: {len(encoded_train_multi)} samples\")\n",
    "print(f\"Encoded validation set: {len(encoded_val_multi)} samples\")\n",
    "print(f\"Encoded test set: {len(encoded_test_multi)} samples\")\n",
    "\n",
    "# Display sample encoding\n",
    "print(f\"\\nSample encoding from train set:\")\n",
    "sample = encoded_train_multi[0]\n",
    "print(f\"  Input IDs length: {len(sample['input_ids'])}\")\n",
    "print(f\"  Attention mask length: {len(sample['attention_mask'])}\")\n",
    "print(f\"  Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trc4ZjKxXphr"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"nlp_assignment\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"  # dont log all model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlAPl4tIRHc0"
   },
   "outputs": [],
   "source": [
    "# Compute class weights for multilingual dataset\n",
    "class_weights_multi = compute_class_weight(\n",
    "    \"balanced\",\n",
    "    classes=np.unique(train_multi[\"label\"].tolist()),\n",
    "    y=train_multi[\"label\"].tolist(),\n",
    ")\n",
    "class_weights_multi = torch.tensor(class_weights_multi, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Custom Weighted Loss Trainer for multilingual model\n",
    "class WeightedLossTrainerMulti(Trainer):\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs=False,\n",
    "        num_items_in_batch=None,  # noqa: ARG002\n",
    "    ):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Move class_weights to the same device as labels\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights_multi.to(labels.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Data collator for multilingual model\n",
    "data_collator_multi = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Store models and trainers for multilingual\n",
    "model_multi_seeds = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\nTraining Multilingual Model with Seed {seed}...\")\n",
    "\n",
    "    # Create multilingual model with sequence classification head\n",
    "    model_multi = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        num_labels=NUM_CLASSES,\n",
    "        id2label={0: \"-\", 1: \"DIRECT\", 2: \"JUDGEMENTAL\", 3: \"REPORTED\"},\n",
    "        label2id={\"-\": 0, \"DIRECT\": 1, \"JUDGEMENTAL\": 2, \"REPORTED\": 3},\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    # Training args for multilingual model\n",
    "    training_args_multi = TrainingArguments(\n",
    "        output_dir=f\"multilingual_model_seed{seed}\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=100,\n",
    "        label_smoothing_factor=0.1,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"BERT-multilingual_seed{seed}\",\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Create weighted loss trainer for multilingual model\n",
    "    trainer_multi = WeightedLossTrainerMulti(\n",
    "        model=model_multi,\n",
    "        args=training_args_multi,\n",
    "        train_dataset=encoded_train_multi,\n",
    "        eval_dataset=encoded_val_multi,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator_multi,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Starting training for seed {seed}...\")\n",
    "    trainer_multi.train()\n",
    "\n",
    "    # Save to HuggingFace Hub\n",
    "    print(f\"Saving Multilingual Model (Seed {seed}) to HuggingFace Hub...\")\n",
    "    model_multi.push_to_hub(\n",
    "        f\"{username}/nlp-transformer-multilingual_seed{seed}\", private=False\n",
    "    )\n",
    "\n",
    "    # Store the trainer\n",
    "    model_multi_seeds[seed] = trainer_multi\n",
    "\n",
    "    wandb.finish()\n",
    "    print(f\"Multilingual Model (Seed {seed}) training and saving completed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTILINGUAL MODEL TRAINING COMPLETE FOR ALL SEEDS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Models saved for seeds: {SEEDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlFt-teUFM5O"
   },
   "outputs": [],
   "source": [
    "multi_eval_acc = pd.read_csv(\"multi_eval_acc.csv\", sep=\";\")\n",
    "multi_train_loss = pd.read_csv(\"multi_train_loss.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnHtI8JEFM5O",
    "outputId": "a76f02f2-3c1e-4c5f-ef4d-4f7cba9da41f"
   },
   "outputs": [],
   "source": [
    "multi_eval_acc_clean = multi_eval_acc.dropna(axis=0)\n",
    "multi_train_loss_clean = multi_train_loss.dropna(axis=0)\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot multilingual evaluation accuracy\n",
    "multi_eval_acc_clean.plot(ax=axes[0], marker=\"o\")\n",
    "axes[0].set_title(\n",
    "    \"Multilingual Model - Evaluation Accuracy Across Seeds\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot multilingual training loss\n",
    "multi_train_loss_clean.plot(ax=axes[1], marker=\"o\")\n",
    "axes[1].set_title(\n",
    "    \"Multilingual Model - Training Loss Across Seeds\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513,
     "referenced_widgets": [
      "be87214fc9fe4061b1927f435925826c",
      "9c3f4564ac2048cf981e4ff4461866ea",
      "72ca7d58b03d4132965e841119468eee",
      "450aa07b53a7409e9049e9f48ae22161",
      "dbb9fa92715f4aefbbc43edea6acb66d",
      "d2594ca451fc46679d619dc5b235666e",
      "dfe1e0dcf9264d01b27408c2563ae1c8",
      "8afd9480a29c43658cfd66704c955da2",
      "5fd1338e5801432e86daa5722079448b",
      "0a6cf2015ea34440bc5eb6aadf9b5a03",
      "b5733f67f35a4a86bdd5a5923af9aa8b",
      "4a9c2fcfc164481ea7870143ceea5c8c",
      "df0c46070b4b483e91b7df431dd170c1",
      "641804b2d1f94ca3adfbb89be3b819db",
      "84d14672db6345009d98440470bcc4f3",
      "5d9dbf1e11404bfaab5fc6a554b581d6",
      "4da155fc07464f66a806c4f86d6e6f82",
      "49101908591d4b3db3da1e2b3c958ea1",
      "5586f49b3c774cf4abd524b83df72b97",
      "e9230dd042104d64af478adf5a4086c6",
      "61d96754a3274fcc841f3a1647939739",
      "b151c81121f744d3b2078cb553144763",
      "23ccebad395a4e32bfea27c7f8b3a187",
      "4e7187330dbf4b489aca407b5ac99917",
      "97f609030e6544ed8b4d9b8b58db3908",
      "86add90267054bbf8cfa8cc5c58ee1f9",
      "d42a65f8eb284aefbaf07bc31ec5efc8",
      "37ae14ab4b8949e5aa69b722642221a1",
      "15d729c84d3f4ffabcab55c6de3fe00c",
      "c081598d540241fba734e14053ed6dc0",
      "02e98390102140eb812e3b030b7edca1",
      "dea85f589bb64f5f8b5f1a9d71079a6f",
      "060b386a4fb043028522fde639f85107",
      "c233a7e467974f568f45099c48f28a5e",
      "e64a8bb3936f4209b56a85d37c7ff00e",
      "3707674001e14af58c07a310a9848993",
      "b684ada25ac64b598ff8af6e025448d7",
      "ba5403a3282241828b28633800646d83",
      "4620850e9059488aa97e8ba2ab2eeaca",
      "005db0f65ff5405c84312b4600d55192",
      "70fbbd27fe2e4812814b193f327bca01",
      "bd63b543d26b4abea69a41b011f451ad",
      "6578e276206340ed87c198997ff893ab",
      "f864d72518b449dca73089b70be475d6",
      "d0da80262d1e412a8e7dcfdec030ee36",
      "35c3ff703fc5482382515102133b455f",
      "157248410396490597f9b1ec851e95db",
      "1a0b4eb3a74b4c3c89d6748af00207e4",
      "bfe64e6cc0704649b45b6aadb8eae19e",
      "ed2a40274df14008bdc8ff3a2c487239",
      "6ac7b0b3bd084c45b6a942a5207233cb",
      "e3eae1fafd0240b09b55caf41dbafe06",
      "bb2cec0488154719b1465b16ca275ad9",
      "d3e122d1bf8b4e2080713dca023b719b",
      "ce2eefa0c8ed4fc3ae75889df541f2ce",
      "6c79ccedf2fb477893d76ca8b8cf55ec",
      "a77a3d1948234294ab56facf9860ce85",
      "f806ee596ac24dfe9c8e7f37070807f1",
      "45afcd54ab9b4c419fb83f202c57c340",
      "432d6bead8b6414dbba5e88bf6225ab7",
      "48fc52476d254dc09d2d66ae0734f41c",
      "799fe8c4eac54b6c8a3674d81d89bb41",
      "6eb965fb50114428b7586e108a588ed9",
      "d93fe497d96c468ba8465bc10477ae3b",
      "1c0dace2b78c4457820093473205590b",
      "65b3c5c7d3144514bfefbedc17c4339a",
      "6f9a9a2e042144f0a2923f4c531e91aa",
      "d45e844ceda0495cafaa3a4ef29fedcf",
      "d11f1ce9a91541848e6e6f830db9ffda",
      "3e0105d3c8f94c108731e8d35853be9f",
      "d7da5f09010a4061b23f58f752971c56",
      "c3ff809137314767be962e080569c545",
      "5659ef7597e04b2b8dc9290a2d028bcc",
      "ae0c0966109445ef8ea4e9cc594d6f74",
      "78aaa9baee4c48b8a7280af6cc205a37",
      "d6f53b8a73434973994691935784b45e",
      "e1ddc7a6d8484894afaba0d9ad2c1f4d",
      "bbe25beafaa54dcfb779f70828f2c5e0",
      "8a46c03156f449369ceb995a6a440b34",
      "511df06a138943bdb6b0c0e2675617ac",
      "7ec9615dddf14aa38a8c7a178e1c531a",
      "8d8957dda81548d49904b39fcf964274",
      "91497bf07cc6443fadec863833260f59",
      "9c93d1d001d44c948cfdabd1fe3f224e",
      "34962cbbfe814192adcb1c809ccfed59",
      "078d2d95473d4b19ac6c1ab84eab813f",
      "5bd8e207294848dd8dcc2174b4b34a5e",
      "b94436f84e3545bfb3bd2cf8b95bfac7",
      "e57c0dd5b339455d90a0284648bc2247",
      "9fe3c50f640b4c6abb3153ad20ece7cf",
      "c756566d129142288a36383122fb5d82",
      "9d391a1264a8446e88077884eff415ed",
      "60ef3132e2f5404fa2c8bf72f52d1fb7",
      "492e4ad5aabd4ef58de38d25e5edabf2",
      "0e33c1e204174376876625bc58dc14a2",
      "fe0f4ffc117c4d6395b75a8009b54ffc",
      "fa1a08f0d564481f92cf9fcd24fec207",
      "d503849563cd475ab5d5bcab5986775c",
      "a740b7925a264e5588a6723402c79e82"
     ]
    },
    "id": "LpxQJ9AtRmAS",
    "outputId": "5b13458b-b9ba-4f0b-9ab6-a049633ecfc2"
   },
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "y_test_multi = test_multi[\"label\"].values\n",
    "seed_metrics_multi = {}\n",
    "seed_predictions_multi = {}\n",
    "loaded_model_identifiers_multi = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n--- Evaluating Multilingual Model - Seed: {seed} ---\")\n",
    "    repo_id_to_load = f\"mpreda/nlp-transformer-multilingual_seed{seed}\"\n",
    "    print(f\"Attempting to load model from: {repo_id_to_load}\")\n",
    "\n",
    "    # Ensure distinct cache directory for each seed\n",
    "    unique_cache_dir = f\"./huggingface_cache_multilingual_seed_{seed}\"\n",
    "    os.makedirs(unique_cache_dir, exist_ok=True)\n",
    "\n",
    "    # Load the multilingual model\n",
    "    model_eval = AutoModelForSequenceClassification.from_pretrained(\n",
    "        repo_id_to_load, cache_dir=unique_cache_dir, force_download=True\n",
    "    )\n",
    "\n",
    "    # Debugging: Get unique identifier for loaded model's weights\n",
    "    if (\n",
    "        hasattr(model_eval, \"classifier\")\n",
    "        and hasattr(model_eval.classifier, \"out_proj\")\n",
    "        and hasattr(model_eval.classifier.out_proj, \"weight\")\n",
    "    ):\n",
    "        classifier_weight_sum = model_eval.classifier.out_proj.weight.sum().item()\n",
    "        print(\n",
    "           f\"Loaded model's classifier.out_proj.weight sum: {classifier_weight_sum:.5f}\"\n",
    "        )\n",
    "        loaded_model_identifiers_multi.append(classifier_weight_sum)\n",
    "    else:\n",
    "        print(\"  Could not retrieve specific weight sum for debugging.\")\n",
    "        loaded_model_identifiers_multi.append(None)\n",
    "\n",
    "    # Ensure model is in evaluation mode and on the correct device\n",
    "    model_eval.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_eval.to(device)\n",
    "\n",
    "    # Manually prepare inputs from encoded_test_multi\n",
    "    input_ids = torch.tensor(encoded_test_multi[\"input_ids\"], dtype=torch.long).to(\n",
    "        device\n",
    "    )\n",
    "    attention_mask = torch.tensor(\n",
    "        encoded_test_multi[\"attention_mask\"], dtype=torch.long\n",
    "    ).to(device)\n",
    "\n",
    "    # Make predictions (get logits) without gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model_eval(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Move logits to CPU and convert to numpy for metric computation\n",
    "    test_predictions_seed_multi = logits.cpu().numpy()\n",
    "    seed_predictions_multi[seed] = test_predictions_seed_multi\n",
    "\n",
    "    # Compute metrics using y_test_multi\n",
    "    seed_metrics_multi[seed] = compute_metrics(\n",
    "        [test_predictions_seed_multi, y_test_multi]\n",
    "    )\n",
    "\n",
    "    # Clean up model and GPU memory\n",
    "    del model_eval\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YWosTNbFM5P",
    "outputId": "3347cba6-5d3f-40bc-d09c-196222281329"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTILINGUAL MODEL - AVERAGE METRICS ACROSS SEEDS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "f1_mean_multi = np.mean([seed_metrics_multi[seed][\"f1\"] for seed in SEEDS])\n",
    "f1_std_multi = np.std([seed_metrics_multi[seed][\"f1\"] for seed in SEEDS])\n",
    "precision_mean_multi = np.mean(\n",
    "    [seed_metrics_multi[seed][\"precision\"] for seed in SEEDS]\n",
    ")\n",
    "precision_std_multi = np.std([seed_metrics_multi[seed][\"precision\"] for seed in SEEDS])\n",
    "recall_mean_multi = np.mean([seed_metrics_multi[seed][\"recall\"] for seed in SEEDS])\n",
    "recall_std_multi = np.std([seed_metrics_multi[seed][\"recall\"] for seed in SEEDS])\n",
    "\n",
    "print(f\"  F1-Score (macro):     {f1_mean_multi:.4f} ± {f1_std_multi:.4f}\")\n",
    "print(f\"  Precision (macro):    {precision_mean_multi:.4f} ± {precision_std_multi:.4f}\")\n",
    "print(f\"  Recall (macro):       {recall_mean_multi:.4f} ± {recall_std_multi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gtiG2mAL3HM"
   },
   "source": [
    "# [Task 7 - 0.5 points] Error Analysis\n",
    "\n",
    "After evaluating the model, perform a brief error analysis on the **test set**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahmga8sz6ogC"
   },
   "source": [
    "### Instructions\n",
    "\n",
    " - Review the results and identify common errors.\n",
    "\n",
    " - Summarize your findings regarding the errors and their impact on performance (e.g. but not limited to Out-of-Vocabulary (OOV) words, data imbalance, and performance differences between the custom model and the transformer...)\n",
    " - Suggest possible solutions to address the identified errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wksBw4HdQb0c",
    "outputId": "2cc705af-77e0-4382-dc19-08e5059e4a0f"
   },
   "outputs": [],
   "source": [
    "# Get test predictions from the best LSTM model\n",
    "base_lstm_model = models[SEEDS[0]][\"baseline_model\"]  # or 'stacked_model' if better\n",
    "stack_lstm_model = models[SEEDS[0]][\"stacked_model\"]\n",
    "\n",
    "base_test_pred = base_lstm_model.predict(test_padded, verbose=0)\n",
    "base_test_pred_classes = np.argmax(base_test_pred, axis=1)\n",
    "\n",
    "stack_test_pred = stack_lstm_model.predict(test_padded, verbose=0)\n",
    "stack_test_pred_classes = np.argmax(stack_test_pred, axis=1)\n",
    "\n",
    "# Get transformer test predictions from the first seed\n",
    "transformer_logits = seed_predictions[SEEDS[0]]\n",
    "transformer_test_pred_classes = np.argmax(transformer_logits, axis=1)\n",
    "\n",
    "print(f\"Base LSTM predictions shape: {base_test_pred_classes.shape}\")\n",
    "print(f\"Stacked LSTM predictions shape: {stack_test_pred_classes.shape}\")\n",
    "print(f\"Transformer predictions shape: {transformer_test_pred_classes.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiusion matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "r9dpJ8R3Qb0c",
    "outputId": "76ac4931-d1cd-4f77-b32f-41a7a999cf86"
   },
   "outputs": [],
   "source": [
    "# Class names\n",
    "class_names = [\"-\", \"DIRECT\", \"JUDGEMENTAL\", \"REPORTED\"]\n",
    "\n",
    "# LSTM Confusion Matrix\n",
    "base_lstm_cm = confusion_matrix(y_test, base_test_pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=base_lstm_cm, display_labels=class_names)\n",
    "disp.plot(ax=ax, cmap=\"Blues\")\n",
    "plt.title(\"Base LSTM Model - Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lstm_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "stack_lstm_cm = confusion_matrix(y_test, stack_test_pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=stack_lstm_cm, display_labels=class_names\n",
    ")\n",
    "disp.plot(ax=ax, cmap=\"Blues\")\n",
    "plt.title(\"Stacked LSTM Model - Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lstm_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Transformer Confusion Matrix\n",
    "transformer_cm = confusion_matrix(y_test, transformer_test_pred_classes)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=transformer_cm, display_labels=class_names\n",
    ")\n",
    "disp.plot(ax=ax, cmap=\"Blues\")\n",
    "plt.title(\"Transformer Model (English-only) - Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"transformer_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Multilingual Transformer Confusion Matrix\n",
    "transformer_multi_logits = seed_predictions_multi[SEEDS[0]]\n",
    "transformer_multi_test_pred_classes = np.argmax(transformer_multi_logits, axis=1)\n",
    "\n",
    "transformer_multi_cm = confusion_matrix(\n",
    "    y_test_multi, transformer_multi_test_pred_classes\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=transformer_multi_cm, display_labels=class_names\n",
    ")\n",
    "disp.plot(ax=ax, cmap=\"Greens\")\n",
    "plt.title(\"Transformer Model (Multilingual EN+ES) - Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"transformer_multi_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall/Precision curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z7tqd9kstMZf",
    "outputId": "3306a14d-e931-4069-dfd6-27ff19ea247e"
   },
   "outputs": [],
   "source": [
    "# Class names\n",
    "class_names = [\"-\", \"DIRECT\", \"JUDGEMENTAL\", \"REPORTED\"]\n",
    "\n",
    "# Binarize test labels for multi-class PR curves\n",
    "y_test_bin = label_binarize(y_test, classes=range(NUM_CLASSES))\n",
    "\n",
    "colors_seeds = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, Orange, Green\n",
    "\n",
    "# Create figures for Baseline and Stacked models\n",
    "fig_baseline, axes_baseline = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig_baseline.suptitle(\n",
    "    \"Baseline LSTM - Precision-Recall Curves (All Seeds)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "fig_stacked, axes_stacked = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig_stacked.suptitle(\n",
    "    \"Stacked LSTM - Precision-Recall Curves (All Seeds)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "axes_base = axes_baseline.ravel()\n",
    "axes_stack = axes_stacked.ravel()\n",
    "\n",
    "# Storage for AP scores\n",
    "ap_data = {\"Seed\": [], \"Model\": [], \"Class\": [], \"AP_Score\": []}\n",
    "\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    for seed_idx, seed in enumerate(SEEDS):\n",
    "        # Get predictions for current seed\n",
    "        baseline_pred = models[seed][\"baseline_model\"].predict(test_padded, verbose=0)\n",
    "        stacked_pred = models[seed][\"stacked_model\"].predict(test_padded, verbose=0)\n",
    "\n",
    "        # Baseline model - Precision-Recall curve\n",
    "        precision_base, recall_base, _ = precision_recall_curve(\n",
    "            y_test_bin[:, class_idx], baseline_pred[:, class_idx]\n",
    "        )\n",
    "        ap_base = average_precision_score(\n",
    "            y_test_bin[:, class_idx], baseline_pred[:, class_idx]\n",
    "        )\n",
    "\n",
    "        # Stacked model - Precision-Recall curve\n",
    "        precision_stack, recall_stack, _ = precision_recall_curve(\n",
    "            y_test_bin[:, class_idx], stacked_pred[:, class_idx]\n",
    "        )\n",
    "        ap_stack = average_precision_score(\n",
    "            y_test_bin[:, class_idx], stacked_pred[:, class_idx]\n",
    "        )\n",
    "\n",
    "        # Plot Baseline PR Curve (Recall vs Precision)\n",
    "        axes_base[class_idx].plot(\n",
    "            recall_base,\n",
    "            precision_base,\n",
    "            label=f\"Seed {seed} (AP={ap_base:.3f})\",\n",
    "            linewidth=2.5,\n",
    "            color=colors_seeds[seed_idx],\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            markevery=max(1, len(recall_base) // 8),\n",
    "        )\n",
    "\n",
    "        # Plot Stacked PR Curve (Recall vs Precision)\n",
    "        axes_stack[class_idx].plot(\n",
    "            recall_stack,\n",
    "            precision_stack,\n",
    "            label=f\"Seed {seed} (AP={ap_stack:.3f})\",\n",
    "            linewidth=2.5,\n",
    "            color=colors_seeds[seed_idx],\n",
    "            marker=\"s\",\n",
    "            markersize=4,\n",
    "            markevery=max(1, len(recall_stack) // 8),\n",
    "        )\n",
    "\n",
    "    # Configure Baseline subplot\n",
    "    axes_base[class_idx].set_xlabel(\n",
    "        \"Recall (Sensitivity)\", fontsize=11, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_base[class_idx].set_ylabel(\"Precision\", fontsize=11, fontweight=\"bold\")\n",
    "    axes_base[class_idx].set_title(\n",
    "        f\"Class: {class_names[class_idx]}\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_base[class_idx].legend(fontsize=9, loc=\"best\")\n",
    "    axes_base[class_idx].grid(True, alpha=0.3)\n",
    "    axes_base[class_idx].set_xlim([0, 1])\n",
    "    axes_base[class_idx].set_ylim([0, 1])\n",
    "\n",
    "    # Configure Stacked subplot\n",
    "    axes_stack[class_idx].set_xlabel(\n",
    "        \"Recall (Sensitivity)\", fontsize=11, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_stack[class_idx].set_ylabel(\"Precision\", fontsize=11, fontweight=\"bold\")\n",
    "    axes_stack[class_idx].set_title(\n",
    "        f\"Class: {class_names[class_idx]}\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_stack[class_idx].legend(fontsize=9, loc=\"best\")\n",
    "    axes_stack[class_idx].grid(True, alpha=0.3)\n",
    "    axes_stack[class_idx].set_xlim([0, 1])\n",
    "    axes_stack[class_idx].set_ylim([0, 1])\n",
    "\n",
    "fig_baseline.tight_layout()\n",
    "\n",
    "fig_stacked.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "04-jKC0aGkOK",
    "outputId": "a8d5febd-11f8-4e68-c380-a307e2af3f29"
   },
   "outputs": [],
   "source": [
    "# Binarize test labels for multi-class PR curves\n",
    "y_test_bin = label_binarize(y_test, classes=range(NUM_CLASSES))\n",
    "\n",
    "colors_seeds = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, Orange, Green\n",
    "\n",
    "# Create figure for Transformer model\n",
    "fig_transformer, axes_transformer = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig_transformer.suptitle(\n",
    "    \"Transformer Model - Precision-Recall Curves (All Seeds)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "axes_trans = axes_transformer.ravel()\n",
    "\n",
    "# Plot precision-recall curves for each class and seed\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    for seed_idx, seed in enumerate(SEEDS):\n",
    "        # Get predictions for current seed - convert logits to probabilities\n",
    "        transformer_logits = seed_predictions[seed]\n",
    "        transformer_probs = softmax(transformer_logits, axis=1)\n",
    "\n",
    "        # Transformer model - Precision-Recall curve\n",
    "        precision_trans, recall_trans, _ = precision_recall_curve(\n",
    "            y_test_bin[:, class_idx], transformer_probs[:, class_idx]\n",
    "        )\n",
    "        ap_trans = average_precision_score(\n",
    "            y_test_bin[:, class_idx], transformer_probs[:, class_idx]\n",
    "        )\n",
    "\n",
    "        # Plot Transformer PR Curve (Recall vs Precision)\n",
    "        axes_trans[class_idx].plot(\n",
    "            recall_trans,\n",
    "            precision_trans,\n",
    "            label=f\"Seed {seed} (AP={ap_trans:.3f})\",\n",
    "            linewidth=2.5,\n",
    "            color=colors_seeds[seed_idx],\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            markevery=max(1, len(recall_trans) // 8),\n",
    "        )\n",
    "\n",
    "    # Configure Transformer subplot\n",
    "    axes_trans[class_idx].set_xlabel(\n",
    "        \"Recall (Sensitivity)\", fontsize=11, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_trans[class_idx].set_ylabel(\"Precision\", fontsize=11, fontweight=\"bold\")\n",
    "    axes_trans[class_idx].set_title(\n",
    "        f\"Class: {class_names[class_idx]}\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_trans[class_idx].legend(fontsize=9, loc=\"best\")\n",
    "    axes_trans[class_idx].grid(True, alpha=0.3)\n",
    "    axes_trans[class_idx].set_xlim([0, 1])\n",
    "    axes_trans[class_idx].set_ylim([0, 1])\n",
    "\n",
    "fig_transformer.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FEuzRR4yFM5Q",
    "outputId": "15f91cb0-db18-4f96-846d-3e5d8b25c351"
   },
   "outputs": [],
   "source": [
    "# Binarize multilingual test labels for multi-class PR curves\n",
    "y_test_multi_bin = label_binarize(y_test_multi, classes=range(NUM_CLASSES))\n",
    "\n",
    "colors_seeds = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, Orange, Green\n",
    "\n",
    "# Create figure for Multilingual Transformer model\n",
    "fig_multilingual, axes_multilingual = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig_multilingual.suptitle(\n",
    "    \"Multilingual Transformer - Precision-Recall Curves (All Seeds)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "axes_multi = axes_multilingual.ravel()\n",
    "\n",
    "# Plot precision-recall curves for each class and seed\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    for seed_idx, seed in enumerate(SEEDS):\n",
    "        # Get predictions for current seed - convert logits to probabilities\n",
    "        multilingual_logits = seed_predictions_multi[seed]\n",
    "        multilingual_probs = softmax(multilingual_logits, axis=1)\n",
    "\n",
    "        # Multilingual model - Precision-Recall curve\n",
    "        precision_multi, recall_multi, _ = precision_recall_curve(\n",
    "            y_test_multi_bin[:, class_idx], multilingual_probs[:, class_idx]\n",
    "        )\n",
    "        ap_multi = average_precision_score(\n",
    "            y_test_multi_bin[:, class_idx], multilingual_probs[:, class_idx]\n",
    "        )\n",
    "\n",
    "        # Plot Multilingual PR Curve (Recall vs Precision)\n",
    "        axes_multi[class_idx].plot(\n",
    "            recall_multi,\n",
    "            precision_multi,\n",
    "            label=f\"Seed {seed} (AP={ap_multi:.3f})\",\n",
    "            linewidth=2.5,\n",
    "            color=colors_seeds[seed_idx],\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            markevery=max(1, len(recall_multi) // 8),\n",
    "        )\n",
    "\n",
    "    # Configure Multilingual subplot\n",
    "    axes_multi[class_idx].set_xlabel(\n",
    "        \"Recall (Sensitivity)\", fontsize=11, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_multi[class_idx].set_ylabel(\"Precision\", fontsize=11, fontweight=\"bold\")\n",
    "    axes_multi[class_idx].set_title(\n",
    "        f\"Class: {class_names[class_idx]}\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes_multi[class_idx].legend(fontsize=9, loc=\"best\")\n",
    "    axes_multi[class_idx].grid(True, alpha=0.3)\n",
    "    axes_multi[class_idx].set_xlim([0, 1])\n",
    "    axes_multi[class_idx].set_ylim([0, 1])\n",
    "\n",
    "fig_multilingual.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction errors respect test lenght analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "POjCIwqxFM5Q",
    "outputId": "519b36ec-1561-454e-ad94-a8e32411063a"
   },
   "outputs": [],
   "source": [
    "# Calculate text lengths for test set\n",
    "test_lengths = test[\"tweet\"].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "# Define length bins\n",
    "length_bins = [0, 20, 50, 100, 500]\n",
    "bin_labels = [\"0-20\", \"20-50\", \"50-100\", \"100+\"]\n",
    "\n",
    "# Create binned data\n",
    "binned_lengths = pd.cut(test_lengths, bins=length_bins, labels=bin_labels, right=False)\n",
    "\n",
    "# Create a dataframe for analysis\n",
    "error_analysis_df = pd.DataFrame(\n",
    "    {\n",
    "        \"length_bin\": binned_lengths,\n",
    "        \"tweet_length\": test_lengths,\n",
    "        \"true_label\": y_test,\n",
    "        \"base_lstm_pred\": base_test_pred_classes,\n",
    "        \"stack_lstm_pred\": stack_test_pred_classes,\n",
    "        \"transformer_pred\": transformer_test_pred_classes,\n",
    "    }\n",
    ")\n",
    "\n",
    "# For multilingual model, use multilingual test data lengths\n",
    "test_lengths_multi = test_multi[\"tweet\"].apply(lambda x: len(word_tokenize(x)))\n",
    "binned_lengths_multi = pd.cut(\n",
    "    test_lengths_multi, bins=length_bins, labels=bin_labels, right=False\n",
    ")\n",
    "\n",
    "error_analysis_df_multi = pd.DataFrame(\n",
    "    {\n",
    "        \"length_bin\": binned_lengths_multi,\n",
    "        \"tweet_length\": test_lengths_multi,\n",
    "        \"true_label\": y_test_multi,\n",
    "        \"transformer_multi_pred\": transformer_multi_test_pred_classes,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate accuracy by length bin for each model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ERROR DISTRIBUTION BY TEXT LENGTH - ENGLISH-ONLY TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "accuracy_by_length = {}\n",
    "\n",
    "for model_name, pred_col in [\n",
    "    (\"Base LSTM\", \"base_lstm_pred\"),\n",
    "    (\"Stacked LSTM\", \"stack_lstm_pred\"),\n",
    "    (\"Transformer\", \"transformer_pred\"),\n",
    "]:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    accuracies = []\n",
    "    support = []\n",
    "\n",
    "    for bin_label in bin_labels:\n",
    "        mask = error_analysis_df[\"length_bin\"] == bin_label\n",
    "        if mask.sum() > 0:\n",
    "            bin_accuracy = (\n",
    "                error_analysis_df[mask][\"true_label\"]\n",
    "                == error_analysis_df[mask][pred_col]\n",
    "            ).mean()\n",
    "            bin_support = mask.sum()\n",
    "            accuracies.append(bin_accuracy)\n",
    "            support.append(bin_support)\n",
    "\n",
    "            print(\n",
    "                f\"  {bin_label} tokens: Accuracy = {bin_accuracy:.4f} (n={bin_support})\"\n",
    "            )\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "            support.append(0)\n",
    "\n",
    "    accuracy_by_length[model_name] = {\"accuracies\": accuracies, \"support\": support}\n",
    "\n",
    "# Calculate accuracy by length bin for multilingual model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ERROR DISTRIBUTION BY TEXT LENGTH - MULTILINGUAL (EN+ES) TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "accuracy_by_length_multi = {}\n",
    "\n",
    "print(f\"\\nTransformer Multilingual:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "accuracies_multi = []\n",
    "support_multi = []\n",
    "\n",
    "for bin_label in bin_labels:\n",
    "    mask = error_analysis_df_multi[\"length_bin\"] == bin_label\n",
    "    if mask.sum() > 0:\n",
    "        bin_accuracy = (\n",
    "            error_analysis_df_multi[mask][\"true_label\"]\n",
    "            == error_analysis_df_multi[mask][\"transformer_multi_pred\"]\n",
    "        ).mean()\n",
    "        bin_support = mask.sum()\n",
    "        accuracies_multi.append(bin_accuracy)\n",
    "        support_multi.append(bin_support)\n",
    "\n",
    "        print(f\"  {bin_label} tokens: Accuracy = {bin_accuracy:.4f} (n={bin_support})\")\n",
    "    else:\n",
    "        accuracies_multi.append(0)\n",
    "        support_multi.append(0)\n",
    "\n",
    "accuracy_by_length_multi[\"Transformer Multilingual\"] = {\n",
    "    \"accuracies\": accuracies_multi,\n",
    "    \"support\": support_multi,\n",
    "}\n",
    "\n",
    "# Visualize error distribution by text length\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Accuracy by length bin (English-only models)\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "x_pos = np.arange(len(bin_labels))\n",
    "width = 0.25\n",
    "\n",
    "for idx, (model_name, color) in enumerate(zip(accuracy_by_length.keys(), colors, strict=False)):  # noqa: E501\n",
    "    accuracies = accuracy_by_length[model_name][\"accuracies\"]\n",
    "    axes[0].bar(\n",
    "        x_pos + idx * width, accuracies, width, label=model_name, color=color, alpha=0.8\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel(\"Tweet Length (tokens)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Accuracy\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Model Accuracy by Text Length (English-only)\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].set_xticks(x_pos + width)\n",
    "axes[0].set_xticklabels(bin_labels)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Accuracy by length bin (Multilingual model)\n",
    "accuracies_multi = accuracy_by_length_multi[\"Transformer Multilingual\"][\"accuracies\"]\n",
    "axes[1].bar(\n",
    "    x_pos,\n",
    "    accuracies_multi,\n",
    "    width=0.5,\n",
    "    label=\"Transformer Multilingual\",\n",
    "    color=\"#d62728\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "axes[1].set_xlabel(\"Tweet Length (tokens)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_title(\n",
    "    \"Model Accuracy by Text Length (Multilingual EN+ES)\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(bin_labels)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed error analysis by length\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ERROR TYPES BY TEXT LENGTH - ENGLISH-ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for bin_label in bin_labels:\n",
    "    mask = error_analysis_df[\"length_bin\"] == bin_label\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\n{bin_label} tokens (n={mask.sum()}):\")\n",
    "\n",
    "        # For transformer model\n",
    "        subset = error_analysis_df[mask]\n",
    "        errors = subset[\"true_label\"] != subset[\"transformer_pred\"]\n",
    "\n",
    "        if errors.sum() > 0:\n",
    "            print(\n",
    "                f\"  Transformer - Errors: {errors.sum()} ({errors.mean() * 100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "            # Show confusion for errors\n",
    "            error_subset = subset[errors]\n",
    "            for true_label_idx, class_name in enumerate(class_names):\n",
    "                mask_true = error_subset[\"true_label\"] == true_label_idx\n",
    "                if mask_true.sum() > 0:\n",
    "                    pred_dist = error_subset[mask_true][\n",
    "                        \"transformer_pred\"\n",
    "                    ].value_counts()\n",
    "                    # Map numeric predictions to class names\n",
    "                    confusion_dict = {\n",
    "                        class_names[int(pred_class)]: count\n",
    "                        for pred_class, count in pred_dist.items()\n",
    "                    }\n",
    "                    print(f\"    {class_name}: confused with {confusion_dict}\")\n",
    "\n",
    "# Detailed error analysis by length (Multilingual)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ERROR TYPES BY TEXT LENGTH - MULTILINGUAL (EN+ES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for bin_label in bin_labels:\n",
    "    mask = error_analysis_df_multi[\"length_bin\"] == bin_label\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\n{bin_label} tokens (n={mask.sum()}):\")\n",
    "\n",
    "        # For multilingual transformer model\n",
    "        subset = error_analysis_df_multi[mask]\n",
    "        errors = subset[\"true_label\"] != subset[\"transformer_multi_pred\"]\n",
    "\n",
    "        if errors.sum() > 0:\n",
    "            print(\n",
    "                f\"  Transformer Multilingual - Errors: {errors.sum()} ({errors.mean() * 100:.1f}%)\"  # noqa: E501\n",
    "            )\n",
    "\n",
    "            # Show confusion for errors\n",
    "            error_subset = subset[errors]\n",
    "            for true_label_idx, class_name in enumerate(class_names):\n",
    "                mask_true = error_subset[\"true_label\"] == true_label_idx\n",
    "                if mask_true.sum() > 0:\n",
    "                    pred_dist = error_subset[mask_true][\n",
    "                        \"transformer_multi_pred\"\n",
    "                    ].value_counts()\n",
    "                    # Map numeric predictions to class names\n",
    "                    confusion_dict = {\n",
    "                        class_names[int(pred_class)]: count\n",
    "                        for pred_class, count in pred_dist.items()\n",
    "                    }\n",
    "                    print(f\"    {class_name}: confused with {confusion_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-class metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "33Yi_OGnQb0d",
    "outputId": "d8b99984-ba5f-4dfd-c167-1d58f00eefc4"
   },
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "base_lstm_report = classification_report(\n",
    "    y_test, base_test_pred_classes, target_names=class_names, output_dict=True\n",
    ")\n",
    "stack_lstm_report = classification_report(\n",
    "    y_test, stack_test_pred_classes, target_names=class_names, output_dict=True\n",
    ")\n",
    "transformer_report = classification_report(\n",
    "    y_test, transformer_test_pred_classes, target_names=class_names, output_dict=True\n",
    ")\n",
    "transformer_multi_report = classification_report(\n",
    "    y_test_multi,\n",
    "    transformer_multi_test_pred_classes,\n",
    "    target_names=class_names,\n",
    "    output_dict=True,\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASE LSTM MODEL - PER-CLASS PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, base_test_pred_classes, target_names=class_names))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STACKED LSTM MODEL - PER-CLASS PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, stack_test_pred_classes, target_names=class_names))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORMER MODEL (ENGLISH-ONLY) - PER-CLASS PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, transformer_test_pred_classes, target_names=class_names\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORMER MODEL (MULTILINGUAL EN+ES) - PER-CLASS PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test_multi, transformer_multi_test_pred_classes, target_names=class_names\n",
    "    )\n",
    ")\n",
    "\n",
    "# Summary comparison\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": class_names,\n",
    "        \"Base_LSTM_F1\": [base_lstm_report[cn][\"f1-score\"] for cn in class_names],\n",
    "        \"Stack_LSTM_F1\": [stack_lstm_report[cn][\"f1-score\"] for cn in class_names],\n",
    "        \"Transformer_EN_F1\": [transformer_report[cn][\"f1-score\"] for cn in class_names],\n",
    "        \"Transformer_Multi_F1\": [\n",
    "            transformer_multi_report[cn][\"f1-score\"] for cn in class_names\n",
    "        ],\n",
    "        \"Base_LSTM_Precision\": [\n",
    "            base_lstm_report[cn][\"precision\"] for cn in class_names\n",
    "        ],\n",
    "        \"Stack_LSTM_Precision\": [\n",
    "            stack_lstm_report[cn][\"precision\"] for cn in class_names\n",
    "        ],\n",
    "        \"Transformer_EN_Precision\": [\n",
    "            transformer_report[cn][\"precision\"] for cn in class_names\n",
    "        ],\n",
    "        \"Transformer_Multi_Precision\": [\n",
    "            transformer_multi_report[cn][\"precision\"] for cn in class_names\n",
    "        ],\n",
    "        \"Base_LSTM_Recall\": [base_lstm_report[cn][\"recall\"] for cn in class_names],\n",
    "        \"Stack_LSTM_Recall\": [stack_lstm_report[cn][\"recall\"] for cn in class_names],\n",
    "        \"Transformer_EN_Recall\": [\n",
    "            transformer_report[cn][\"recall\"] for cn in class_names\n",
    "        ],\n",
    "        \"Transformer_Multi_Recall\": [\n",
    "            transformer_multi_report[cn][\"recall\"] for cn in class_names\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cjpmzbWKQb0d",
    "outputId": "14eb4983-fe99-49c8-f63c-97f82a728cc0"
   },
   "outputs": [],
   "source": [
    "# Class distribution analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS - ENGLISH-ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_dist = train[\"label\"].value_counts().sort_index()\n",
    "val_dist = val[\"label\"].value_counts().sort_index()\n",
    "test_dist = test[\"label\"].value_counts().sort_index()\n",
    "\n",
    "dist_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": class_names,\n",
    "        \"Train\": [train_dist.get(i, 0) for i in range(4)],\n",
    "        \"Val\": [val_dist.get(i, 0) for i in range(4)],\n",
    "        \"Test\": [test_dist.get(i, 0) for i in range(4)],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dist_df.to_string(index=False))\n",
    "\n",
    "# Class distribution for multilingual\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS - MULTILINGUAL (EN+ES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_dist_multi = train_multi[\"label\"].value_counts().sort_index()\n",
    "val_dist_multi = val_multi[\"label\"].value_counts().sort_index()\n",
    "test_dist_multi = test_multi[\"label\"].value_counts().sort_index()\n",
    "\n",
    "dist_df_multi = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": class_names,\n",
    "        \"Train\": [train_dist_multi.get(i, 0) for i in range(4)],\n",
    "        \"Val\": [val_dist_multi.get(i, 0) for i in range(4)],\n",
    "        \"Test\": [test_dist_multi.get(i, 0) for i in range(4)],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dist_df_multi.to_string(index=False))\n",
    "\n",
    "# Visualize imbalance for both English and Multilingual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# English-only distribution\n",
    "dist_df.set_index(\"Class\")[[\"Train\", \"Val\", \"Test\"]].plot(kind=\"bar\", ax=axes[0])\n",
    "axes[0].set_title(\"Class Distribution (English-only)\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Number of Samples\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].legend(title=\"Dataset\")\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Multilingual distribution\n",
    "dist_df_multi.set_index(\"Class\")[[\"Train\", \"Val\", \"Test\"]].plot(kind=\"bar\", ax=axes[1])\n",
    "axes[1].set_title(\n",
    "    \"Class Distribution (Multilingual EN+ES)\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].set_ylabel(\"Number of Samples\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].legend(title=\"Dataset\")\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"class_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Check if imbalance affects minority classes\n",
    "print(\"\\nClass Distribution Percentages (English-only):\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    test_pct = 100 * test_dist.get(i, 0) / len(test)\n",
    "    print(f\"  {cls}: {test_pct:.1f}%\")\n",
    "\n",
    "print(\"\\nClass Distribution Percentages (Multilingual EN+ES):\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    test_pct = 100 * test_dist_multi.get(i, 0) / len(test_multi)\n",
    "    print(f\"  {cls}: {test_pct:.1f}%\")\n",
    "\n",
    "print(\"\\nDataset Sizes:\")\n",
    "print(f\"  English-only - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "print(\n",
    "    f\"  Multilingual (EN+ES) - Train: {len(train_multi)}, Val: {len(val_multi)}, Test: {len(test_multi)}\"  # noqa: E501\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3jccyPI0Qb0e",
    "outputId": "6feccd1f-531a-4431-ece6-7f5596449de9"
   },
   "outputs": [],
   "source": [
    "# OOV analysis for all three models\n",
    "unk_token_idx = vocab[\"<UNK>\"]\n",
    "\n",
    "\n",
    "def count_unk_in_sample(encoded_indices):\n",
    "    \"\"\"Count <UNK> tokens in a sample\"\"\"\n",
    "    return sum(1 for idx in encoded_indices if idx == unk_token_idx)\n",
    "\n",
    "\n",
    "test[\"unk_count\"] = test[\"encoded_tweet\"].apply(count_unk_in_sample)\n",
    "\n",
    "# Note: Multilingual model uses transformer tokenizer (subword tokens),\n",
    "# not LSTM vocabulary. So OOV analysis is not directly comparable\n",
    "\n",
    "# Generate predictions for all three models on test set if not already done\n",
    "# Baseline model\n",
    "base_test_pred = models[42][\"baseline_model\"].predict(test_padded, verbose=0)\n",
    "base_test_pred_classes = np.argmax(base_test_pred, axis=1)\n",
    "\n",
    "# Stacked model\n",
    "stack_test_pred = models[42][\"stacked_model\"].predict(test_padded, verbose=0)\n",
    "stack_test_pred_classes = np.argmax(stack_test_pred, axis=1)\n",
    "\n",
    "# Transformer (already have this)\n",
    "# transformer_test_pred_classes is already computed\n",
    "\n",
    "# Compute error masks for all three models (English-only)\n",
    "base_errors = y_test != base_test_pred_classes\n",
    "stack_errors = y_test != stack_test_pred_classes\n",
    "transformer_errors = y_test != transformer_test_pred_classes\n",
    "\n",
    "# Multilingual transformer errors\n",
    "transformer_multi_errors = y_test_multi != transformer_multi_test_pred_classes\n",
    "\n",
    "# Analyze error rate by OOV (English-only)\n",
    "high_oov = test[\"unk_count\"] > 5  # noqa: PLR2004\n",
    "low_oov = test[\"unk_count\"] <= 5  # noqa: PLR2004\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OOV (OUT-OF-VOCABULARY) ANALYSIS - ENGLISH-ONLY TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Samples with high OOV (>5 <UNK> tokens): {high_oov.sum()}\")\n",
    "print(f\"Samples with low OOV (<=5 <UNK> tokens): {low_oov.sum()}\")\n",
    "\n",
    "# Compute error rates for each model\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ERROR RATES BY OOV LEVEL\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "models_info = {\n",
    "    \"Baseline LSTM\": base_errors,\n",
    "    \"Stacked LSTM\": stack_errors,\n",
    "    \"Transformer\": transformer_errors,\n",
    "}\n",
    "\n",
    "oov_analysis = []\n",
    "\n",
    "for model_name, error_mask in models_info.items():\n",
    "    if high_oov.sum() > 0:\n",
    "        error_rate_high_oov = error_mask[high_oov].sum() / high_oov.sum()\n",
    "    else:\n",
    "        error_rate_high_oov = 0\n",
    "\n",
    "    if low_oov.sum() > 0:\n",
    "        error_rate_low_oov = error_mask[low_oov].sum() / low_oov.sum()\n",
    "    else:\n",
    "        error_rate_low_oov = 0\n",
    "\n",
    "    diff = error_rate_high_oov - error_rate_low_oov\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Error rate (high OOV):  {100 * error_rate_high_oov:.1f}%\")\n",
    "    print(f\"  Error rate (low OOV):   {100 * error_rate_low_oov:.1f}%\")\n",
    "    print(f\"  Difference:             {100 * diff:.1f}%\")\n",
    "\n",
    "    oov_analysis.append(\n",
    "        {\n",
    "            \"Model\": model_name,\n",
    "            \"High OOV Error Rate\": f\"{100 * error_rate_high_oov:.1f}%\",\n",
    "            \"Low OOV Error Rate\": f\"{100 * error_rate_low_oov:.1f}%\",\n",
    "            \"OOV Impact\": f\"{100 * diff:.1f}%\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY TABLE (ENGLISH-ONLY)\")\n",
    "print(\"=\" * 70)\n",
    "oov_summary_df = pd.DataFrame(oov_analysis)\n",
    "print(oov_summary_df.to_string(index=False))\n",
    "\n",
    "# Visualize OOV distribution for English-only\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "test[\"unk_count\"].hist(bins=30, ax=ax, edgecolor=\"black\", alpha=0.7, color=\"#1f77b4\")\n",
    "ax.set_title(\"OOV Distribution - English-only Test Set\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Number of <UNK> Tokens per Sample\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"oov_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOOV Statistics (English-only):\")\n",
    "print(f\"  Average <UNK> tokens per sample: {test['unk_count'].mean():.2f}\")\n",
    "print(f\"  Max <UNK> tokens in a sample: {test['unk_count'].max()}\")\n",
    "print(f\"  Min <UNK> tokens in a sample: {test['unk_count'].min()}\")\n",
    "\n",
    "# Multilingual model information\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTILINGUAL MODEL - TOKENIZATION NOTE\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    \"The multilingual transformer model uses BERT multilingual tokenizer (subword tokens)\"  # noqa: E501\n",
    ")\n",
    "print(\n",
    "    \"rather than LSTM word-level tokenization, so OOV analysis is not directly comparable.\"  # noqa: E501\n",
    ")\n",
    "print(\n",
    "    f\"\\nTransformer Multilingual Error Rate: {(transformer_multi_errors.sum() / len(transformer_multi_errors)) * 100:.1f}%\"  # noqa: E501\n",
    ")\n",
    "print(\n",
    "    f\"Transformer English-only Error Rate: {(transformer_errors.sum() / len(transformer_errors)) * 100:.1f}%\"  # noqa: E501\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P42XYjb6K3k5"
   },
   "source": [
    "# [Task 8 - 0.5 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9oXSaW1K5S7"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHw2L6PlLDyE"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is **not a copy-paste** of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMUqh1utLflM"
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqp8z3MP6ogD"
   },
   "source": [
    "## Bonus Points\n",
    "Bonus points are arbitrarily assigned based on significant contributions such as:\n",
    "- Outstanding error analysis\n",
    "- Masterclass code organization\n",
    "- Suitable extensions\n",
    "\n",
    "**Note**: bonus points are only assigned if all task points are attributed (i.e., 6/6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1KooOpK6ogD"
   },
   "source": [
    "**Possible Suggestions for Bonus Points:**\n",
    "- **Try other preprocessing strategies**: e.g., but not limited to, explore techniques tailored specifically for tweets or  methods that are common in social media text.\n",
    "- **Experiment with other custom architectures or models from HuggingFace**\n",
    "- **Explore Spanish tweets**: e.g., but not limited to, leverage multilingual models to process Spanish tweets and assess their performance compared to monolingual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypagJed7LheY"
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BjMk5e_M4n7"
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8TVgpYlM6s5"
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia6IapI1M_A7"
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1WcrpemNEQm"
   },
   "source": [
    "### Robust Evaluation\n",
    "\n",
    "Each model is trained with at least 3 random seeds.\n",
    "\n",
    "Task 5 requires you to compute the average performance over the 3 seeds and its corresponding standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRQsR3ti6ogE"
   },
   "source": [
    "### Expected Results\n",
    "\n",
    "Task 2 leaderboard reports around 40-50 F1-score.\n",
    "However, note that they perform a hierarchical classification.\n",
    "\n",
    "That said, results around 30-40 F1-score are **expected** given the task's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mVe5dqzNI_u"
   },
   "source": [
    "### Model Selection for Analysis\n",
    "\n",
    "To carry out the error analysis you are **free** to either\n",
    "\n",
    "* Pick examples or perform comparisons with an individual seed run model (e.g., Baseline seed 1337)\n",
    "* Perform ensembling via, for instance, majority voting to obtain a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8a4pDKSNKzI"
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xmMKE7vLu-y"
   },
   "source": [
    "\n",
    "# The End\n",
    "\n",
    "Feel free to reach out for questions/doubts!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
